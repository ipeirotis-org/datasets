{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# !sudo apt-get update\n",
    "# !sudo apt-get install python3-rtree\n",
    "# !sudo pip3 install -U geopandas descartes pandas matplotlib\n",
    "# !sudo pip3 install -U shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "t_start = datetime.now()\n",
    "\n",
    "t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Render our plots inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "!curl 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD' -o accidents.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "df = pd.read_csv(\"accidents.csv\", low_memory=False, dtype='object')\n",
    "df = pd.DataFrame(df[0:10000])\n",
    "df['DATETIME'] = df.DATE + ' ' + df.TIME\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'], format=\"%m/%d/%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "!rm accidents.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df.rename(\n",
    "    {\n",
    "    \"ZIP CODE\" : 'ZIPCODE',\n",
    "    \"NUMBER OF PERSONS INJURED\": \"PERSONS_INJURED\",\n",
    "    \"NUMBER OF PERSONS KILLED\": \"PERSONS_KILLED\",\n",
    "    \"NUMBER OF PEDESTRIANS INJURED\": \"PEDESTRIANS_INJURED\",\n",
    "    \"NUMBER OF PEDESTRIANS KILLED\": \"PEDESTRIANS_KILLED\",\n",
    "    'NUMBER OF MOTORIST INJURED': 'MOTORISTS_INJURED',\n",
    "    'NUMBER OF MOTORIST KILLED': 'MOTORISTS_KILLED', \n",
    "    'NUMBER OF CYCLIST INJURED': 'CYCLISTS_INJURED', \n",
    "    'NUMBER OF CYCLIST KILLED': 'CYCLISTS_KILLED',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 1': 'CAUSE_VEHICLE_1',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 2': 'CAUSE_VEHICLE_2',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 3': 'CAUSE_VEHICLE_3',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 4': 'CAUSE_VEHICLE_4',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 5': 'CAUSE_VEHICLE_5',\n",
    "    'VEHICLE TYPE CODE 1': 'TYPE_VEHICLE_1',\n",
    "    'VEHICLE TYPE CODE 2': 'TYPE_VEHICLE_2',\n",
    "    'VEHICLE TYPE CODE 3': 'TYPE_VEHICLE_3',\n",
    "    'VEHICLE TYPE CODE 4': 'TYPE_VEHICLE_4',\n",
    "    'VEHICLE TYPE CODE 5': 'TYPE_VEHICLE_5',\n",
    "    },\n",
    "    axis = 'columns',\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "cols = df.columns\n",
    "cols = cols.map(lambda x: x.replace(' ', '_'))\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "cols = [\n",
    "    'UNIQUE_KEY',\n",
    "    'DATETIME',\n",
    "    'BOROUGH',\n",
    "    'ZIPCODE',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    'LOCATION',\n",
    "    'ON_STREET_NAME',\n",
    "    'CROSS_STREET_NAME',\n",
    "    'OFF_STREET_NAME',\n",
    "    'PERSONS_INJURED',\n",
    "    'PERSONS_KILLED',\n",
    "    'PEDESTRIANS_INJURED',\n",
    "    'PEDESTRIANS_KILLED',\n",
    "    'CYCLISTS_INJURED',\n",
    "    'CYCLISTS_KILLED',\n",
    "    'MOTORISTS_INJURED',\n",
    "    'MOTORISTS_KILLED',\n",
    "    'CAUSE_VEHICLE_1',\n",
    "    'CAUSE_VEHICLE_2',\n",
    "    'CAUSE_VEHICLE_3',\n",
    "    'CAUSE_VEHICLE_4',\n",
    "    'CAUSE_VEHICLE_5',\n",
    "    'TYPE_VEHICLE_1',\n",
    "    'TYPE_VEHICLE_2',\n",
    "    'TYPE_VEHICLE_3',\n",
    "    'TYPE_VEHICLE_4',\n",
    "    'TYPE_VEHICLE_5',\n",
    "]\n",
    "\n",
    "df = df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Analysis of Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# df['DATETIME'] = df.DATE + ' ' + df.TIME\n",
    "# df.DATETIME = pd.to_datetime(df.DATETIME, format=\"%m/%d/%Y %H:%M\")\n",
    "#df.TIME = pd.to_datetime(df.TIME, format=\"%H:%M\")\n",
    "#df.DATE = pd.to_datetime(df.DATE, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# df['DATETIME'].hist(bins=7*12, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# df.drop( ['DATE','TIME'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['BOROUGH'] = pd.Categorical(df.BOROUGH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['BOROUGH'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "sum(~df['BOROUGH'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Significant number of missing values\n",
    "# We will fix later, by joining with the NYC neighborhood shapefile\n",
    "sum(df['BOROUGH'].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['ZIPCODE'] = pd.Categorical(df['ZIPCODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Significant number of missing values\n",
    "# We will fix later, by joining with the NYC ZIP shapefile\n",
    "sum(df['ZIPCODE'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "# We will get back to these columns with geo-shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Longitute and Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df.LATITUDE = pd.to_numeric(df.LATITUDE)\n",
    "df.LONGITUDE  = pd.to_numeric(df.LONGITUDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "sum(df['LATITUDE'].isnull() | df['LONGITUDE'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['LATITUDE'].fillna(0.0, inplace=True)\n",
    "df['LONGITUDE'].fillna(0.0, inplace=True)\n",
    "\n",
    "# We keep LOCATION (which is largely redundant), just to make easy\n",
    "# analysis of errors later on. \n",
    "df.LOCATION = '(' + df.LATITUDE.astype(float).astype(str) + ', ' + df.LONGITUDE.astype(float).astype(str) + ')'\n",
    "\n",
    "# This is just lon/lat combined, we could drop it, but we will drop it later\n",
    "# df.drop( ['LOCATION'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Numeric Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# df['UNIQUE_KEY'] = pd.to_numeric(df['UNIQUE_KEY'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['PERSONS_INJURED'].fillna(0, inplace=True)\n",
    "df['PERSONS_INJURED'] = pd.to_numeric(df['PERSONS_INJURED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['PERSONS_KILLED'].fillna(0, inplace=True)\n",
    "df['PERSONS_KILLED']  = pd.to_numeric(df['PERSONS_KILLED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['PEDESTRIANS_INJURED'].fillna(0, inplace=True)\n",
    "df['PEDESTRIANS_INJURED'] = pd.to_numeric(df['PEDESTRIANS_INJURED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['PEDESTRIANS_KILLED'].fillna(0, inplace=True)\n",
    "df['PEDESTRIANS_KILLED'] = pd.to_numeric(df['PEDESTRIANS_KILLED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['CYCLISTS_INJURED'].fillna(0, inplace=True)\n",
    "df['CYCLISTS_INJURED'] = pd.to_numeric(df['CYCLISTS_INJURED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['CYCLISTS_KILLED'].fillna(0, inplace=True)\n",
    "df['CYCLISTS_KILLED'] = pd.to_numeric(df['CYCLISTS_KILLED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['MOTORISTS_INJURED'].fillna(0, inplace=True)\n",
    "df['MOTORISTS_INJURED'] = pd.to_numeric(df['MOTORISTS_INJURED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['MOTORISTS_KILLED'].fillna(0, inplace=True)\n",
    "df['MOTORISTS_KILLED'] = pd.to_numeric(df['MOTORISTS_KILLED'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Normalizing Causes and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df['CAUSE_VEHICLE_1'] = pd.Categorical(df['CAUSE_VEHICLE_1'].str.upper())\n",
    "df['CAUSE_VEHICLE_2'] = pd.Categorical(df['CAUSE_VEHICLE_2'].str.upper())\n",
    "df['CAUSE_VEHICLE_3'] = pd.Categorical(df['CAUSE_VEHICLE_3'].str.upper())\n",
    "df['CAUSE_VEHICLE_4'] = pd.Categorical(df['CAUSE_VEHICLE_4'].str.upper())\n",
    "df['CAUSE_VEHICLE_5'] = pd.Categorical(df['CAUSE_VEHICLE_5'].str.upper())\n",
    "df['TYPE_VEHICLE_1'] = pd.Categorical(df['TYPE_VEHICLE_1'].str.upper())\n",
    "df['TYPE_VEHICLE_2'] = pd.Categorical(df['TYPE_VEHICLE_2'].str.upper())\n",
    "df['TYPE_VEHICLE_3'] = pd.Categorical(df['TYPE_VEHICLE_3'].str.upper())\n",
    "df['TYPE_VEHICLE_4'] = pd.Categorical(df['TYPE_VEHICLE_4'].str.upper())\n",
    "df['TYPE_VEHICLE_5'] = pd.Categorical(df['TYPE_VEHICLE_5'].str.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "df1 = df[['UNIQUE_KEY', 'CAUSE_VEHICLE_1', 'TYPE_VEHICLE_1']].copy()\n",
    "df1['VEHICLE'] = 1\n",
    "df1.columns = ['UNIQUE_KEY', 'CAUSE', 'VEHICLE_TYPE', 'VEHICLE']\n",
    "\n",
    "df2 = df[['UNIQUE_KEY', 'CAUSE_VEHICLE_2', 'TYPE_VEHICLE_2']].copy()\n",
    "df2['VEHICLE'] = 2\n",
    "df2.columns = ['UNIQUE_KEY', 'CAUSE', 'VEHICLE_TYPE', 'VEHICLE']\n",
    "\n",
    "df3 = df[['UNIQUE_KEY', 'CAUSE_VEHICLE_3', 'TYPE_VEHICLE_3']].copy()\n",
    "df3['VEHICLE'] = 3\n",
    "df3.columns = ['UNIQUE_KEY', 'CAUSE', 'VEHICLE_TYPE', 'VEHICLE']\n",
    "\n",
    "df4 = df[['UNIQUE_KEY', 'CAUSE_VEHICLE_4', 'TYPE_VEHICLE_4']].copy()\n",
    "df4['VEHICLE'] = 4\n",
    "df4.columns = ['UNIQUE_KEY', 'CAUSE', 'VEHICLE_TYPE', 'VEHICLE']\n",
    "\n",
    "df5 = df[['UNIQUE_KEY', 'CAUSE_VEHICLE_5', 'TYPE_VEHICLE_5']].copy()\n",
    "df5['VEHICLE'] = 5\n",
    "df5.columns = ['UNIQUE_KEY', 'CAUSE', 'VEHICLE_TYPE', 'VEHICLE']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "concat_df = [df1, df2, df3, df4, df5]\n",
    "ct_df = pd.concat(concat_df)\n",
    "ct_df = ct_df.set_index('UNIQUE_KEY')\n",
    "ct_df = ct_df.sort_values(['UNIQUE_KEY', 'VEHICLE'])\n",
    "ct_df = ct_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# ct_df['CAUSE'] = pd.Categorical(ct_df['CAUSE'])\n",
    "# ct_df['VEHICLE_TYPE'] = pd.Categorical(ct_df['VEHICLE_TYPE'])\n",
    "# ct_df['VEHICLE'] = pd.Categorical(ct_df['VEHICLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Since we have the ct_df (\"causes/types dataframe\") we can drop these columns\n",
    "# from the main dataframe\n",
    "todrop = [\n",
    "    'CAUSE_VEHICLE_1', 'TYPE_VEHICLE_1', \n",
    "    'CAUSE_VEHICLE_2', 'TYPE_VEHICLE_2',\n",
    "    'CAUSE_VEHICLE_3', 'TYPE_VEHICLE_3', \n",
    "    'CAUSE_VEHICLE_4', 'TYPE_VEHICLE_4',\n",
    "    'CAUSE_VEHICLE_5', 'TYPE_VEHICLE_5'\n",
    "]\n",
    "df.drop(todrop, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Detecting Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Incorrect number for injured people\n",
    "check = (df.PEDESTRIANS_INJURED + df.CYCLISTS_INJURED + df.MOTORISTS_INJURED != df.PERSONS_INJURED)\n",
    "incorrect_injured = set(df[check].UNIQUE_KEY.values)\n",
    "len(incorrect_injured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "# Incorrect number of people killed\n",
    "check = ( df.PEDESTRIANS_KILLED + df.CYCLISTS_KILLED + df.MOTORISTS_KILLED != df.PERSONS_KILLED)\n",
    "incorrect_killed = set(df[check].UNIQUE_KEY.values)\n",
    "len(incorrect_killed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# No entries for vehicle/cause\n",
    "nocause = set(df.UNIQUE_KEY.values) -set(ct_df.index.values)\n",
    "len(nocause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Incorrect number of entries for vehicle types/causes \n",
    "dd1 = ct_df[['VEHICLE']].groupby('UNIQUE_KEY').count() # number of vehicles involved\n",
    "dd2 = ct_df[['VEHICLE']].groupby('UNIQUE_KEY').max() # higher number of VEHICLE_NUM\n",
    "j = dd1.merge(dd2, left_index=True, right_index=True)\n",
    "j.columns = ['cnt', 'max']\n",
    "# j.query(\"cnt != max\")\n",
    "\n",
    "incorrect_vehicles = set(j.query(\"cnt != max\").index.values)\n",
    "len(incorrect_vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "todelete = incorrect_injured | incorrect_killed | nocause | incorrect_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df[ df.UNIQUE_KEY.isin(todelete) ].pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df = df[ ~df.UNIQUE_KEY.isin(todelete) ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Lon/lat analysis, plus zipcode/borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The 'mask' sets rough bound box limits for NYC\n",
    "mask = (df.LATITUDE > 40) & (df.LATITUDE < 41) & (df.LONGITUDE < -72) & (df.LONGITUDE > -74.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "len(df[ ~mask ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(df[ mask ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Check if there is a temporal pattern in the errors\n",
    "# where lon/lat are clearly not within NYC boundaries\n",
    "# They are mostly missing ie. location = (0.0,0.0)\n",
    "# The spikes are visible for the period of Mar-May 2016\n",
    "# While we will drop these, it would be good to re-examine\n",
    "# these later on\n",
    "df[ ~mask ].pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# We have a dillema here: If we drop the incorrect lon/lat values\n",
    "# then we introduce clear abnormalities in the number of accidents over time\n",
    "df[ mask ].pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# FIXABLE error\n",
    "# We have lon/lat within the NYC boundaries, but no borough\n",
    "len(df[ mask & df.BOROUGH.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# FIXABLE error\n",
    "# We have lon/lat within the NYC boundaries, but no ZIP\n",
    "len(df[ mask & df.ZIPCODE.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# No cases where BOROUGH is null but zipcode is not\n",
    "df[ df.BOROUGH.isnull() & ~df.ZIPCODE.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Small number of cases where the borough is missing but not zip\n",
    "len(df[ ~df.BOROUGH.isnull() & df.ZIPCODE.isnull() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# A map of all the accidents where the BOROUGH is NULL\n",
    "# We do not detect any obvious spatial patterns of missingness\n",
    "# at least visually\n",
    "\n",
    "\n",
    "df[ df.BOROUGH.isnull() & mask ].plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    figsize=(20, 15),\n",
    "    s=0.5,\n",
    "    alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Check if there is a temporal pattern in the errors\n",
    "\n",
    "df[ df.BOROUGH.isnull() & mask ].pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Augment Borough and Zipcode using Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "# We start by converting the LON/LAT attributes into Shapely \"geo-points\"\n",
    "# and convert the resut into a geodataframe so that we can do a spatial join \n",
    "from shapely.geometry import Point\n",
    "df['Coordinates'] = list(zip(df.LONGITUDE, df.LATITUDE))\n",
    "df['Coordinates'] = df['Coordinates'].apply(Point)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='Coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# We will now perform a spatial join with the NYC shapefile \n",
    "# to find the neighborhood for each accident, and also filter out\n",
    "# automatically all the accidents that have incorrect coordinates\n",
    "# and/or incorrect BOROUGH listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "shapefile_url = 'https://data.cityofnewyork.us/api/geospatial/cpf4-rkhq?method=export&format=Shapefile'\n",
    "df_nyc = gpd.GeoDataFrame.from_file(shapefile_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "df_nyc.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "# Match each accident with a neighborhood. \n",
    "# Takes ~ 6-7 mins to run\n",
    "# This is done with left join, \n",
    "# so we preserve all the data points\n",
    "# but we know which ones are not matching with the shapefile\n",
    "gdf.crs = df_nyc.crs\n",
    "gdf = gpd.sjoin(gdf, df_nyc, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# We keep only boro_name and ntaname\n",
    "todrop = [\n",
    "    'index_right', 'boro_code', 'county_fip', 'ntacode',\n",
    "    'shape_area', 'shape_leng'\n",
    "]\n",
    "\n",
    "gdf = gdf.drop(todrop, axis='columns')\n",
    "\n",
    "# Rename the columns\n",
    "gdf = gdf.rename({\n",
    "    'boro_name': 'GEO_BOROUGH',\n",
    "    'ntaname': 'GEO_NEIGHBORHOOD',\n",
    "},\n",
    "                 axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "gdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "%%time\n",
    "shapefile_zip = 'https://data.cityofnewyork.us/download/i8iw-xf4u/application%2Fzip'\n",
    "df_zip = gpd.GeoDataFrame.from_file(shapefile_zip)\n",
    "# Converting the df_zip from {'init': 'epsg:2263'} coordinate system\n",
    "# to the same lon/lat system used by df_nyc\n",
    "df_zip = df_zip.to_crs(df_nyc.crs)\n",
    "\n",
    "# Creating the geodataframe using lon/lat coordinates\n",
    "# gdf['Coordinates'] = list(zip(gdf.LONGITUDE, gdf.LATITUDE))\n",
    "# gdf['Coordinates'] = gdf['Coordinates'].apply(Point)\n",
    "# gdf = gpd.GeoDataFrame(gdf, geometry='Coordinates')\n",
    "\n",
    "# Spatial inner join, keeping \n",
    "gdf.crs = df_zip.crs\n",
    "gdf = gpd.sjoin(gdf, df_zip, how='left')\n",
    "\n",
    "todrop = [\n",
    "    'BLDGZIP', 'PO_NAME', 'POPULATION', 'AREA', 'STATE', 'COUNTY', 'ST_FIPS',\n",
    "    'CTY_FIPS', 'URL', 'SHAPE_AREA', 'SHAPE_LEN', 'index_right'\n",
    "]\n",
    "gdf = gdf.drop(todrop, axis='columns')\n",
    "gdf = gdf.rename({'ZIPCODE_left': 'ZIPCODE', 'ZIPCODE_right': 'GEO_ZIPCODE'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "gdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Post-Augmentation Analysis of Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Ensure that all collisions are in place\n",
    "not_matching = set(df['UNIQUE_KEY'].values) - set(gdf['UNIQUE_KEY'].values)\n",
    "assert( len(not_matching) ==0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Did not match with a shape within the NYC Neighborhoods\n",
    "len(gdf[ gdf.GEO_BOROUGH.isnull() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Did not match with a shape within the NYC ZIP codes\n",
    "len(gdf[ gdf.GEO_ZIPCODE.isnull() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Did not match with neither zip nor neighborhood\n",
    "len(gdf[ gdf.GEO_ZIPCODE.isnull() & gdf.GEO_BOROUGH.isnull() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(gdf[ gdf.GEO_ZIPCODE.isnull() & ~gdf.GEO_BOROUGH.isnull() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(gdf[ ~gdf.GEO_ZIPCODE.isnull() & gdf.GEO_BOROUGH.isnull() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Outside general NYC area\n",
    "\n",
    "These are mainly the entries with no lon/lat, or very incorrect lon/lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Defines  LON/LAT entries that are plausibly within NYC\n",
    "mask = (gdf.LATITUDE > 40) & (gdf.LATITUDE < 41) & (gdf.LONGITUDE < -72) & (gdf.LONGITUDE > -74.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# These are the entries that we could filter out with a simple mask\n",
    "nm1 = gdf[ gdf.GEO_BOROUGH.isnull() & ~mask ]\n",
    "nm1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm1.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm1.LOCATION.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### No borough detected, within NYC boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# These are the entries that are not matching with a neighborhood\n",
    "# but are within the NYC boundaries.\n",
    "nm2 = gdf[gdf.GEO_BOROUGH.isnull() & mask  ]\n",
    "len(nm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm2.LOCATION.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm2['ON_STREET_NAME'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm2.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The plot reveals that these are all bridge accidents\n",
    "#\n",
    "# Perhaps we may want to keep them in later versions\n",
    "#\n",
    "base = df_nyc.plot(linewidth=0.5,\n",
    "                   color='White',\n",
    "                   edgecolor='Black',\n",
    "                   figsize=(15, 12),\n",
    "                   alpha=0.75)\n",
    "\n",
    "nm2.plot(figsize=(15, 12), c='red', markersize=2, alpha=0.25, ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm2.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### No ZIP detected, within NYC boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# These are the entries that are not matching with a neighborhood\n",
    "# but are within the NYC boundaries.\n",
    "nm3 = gdf[gdf.GEO_ZIPCODE.isnull() & mask  ]\n",
    "len(nm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm3['ON_STREET_NAME'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The plot reveals that these are all bridge accidents\n",
    "#\n",
    "# Perhaps we may want to keep them in later versions\n",
    "#\n",
    "base = df_zip.plot(linewidth=0.5,\n",
    "                   color='White',\n",
    "                   edgecolor='Black',\n",
    "                   figsize=(15, 12),\n",
    "                   alpha=0.75)\n",
    "\n",
    "nm3.plot(figsize=(15, 12), c='red', markersize=2, alpha=0.25, ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "nm3.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Discrepancy between detected and reported borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Code below lists cases where the borough is incorrectly coded based on lon/lat\n",
    "# TODO: Perhaps also check cases where zipcode is incorrectly coded\n",
    "errors_boro = gdf[mask & ~gdf.BOROUGH.isnull() & ~gdf.GEO_BOROUGH.isnull() &\n",
    "                  (gdf.GEO_BOROUGH.str.lower() != gdf.BOROUGH.str.lower())]\n",
    "errors_boro = pd.DataFrame(errors_boro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(errors_boro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "errors_boro.LOCATION.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The plot reveals that most are border issues\n",
    "# but there are random errors spread throughout the city\n",
    "base = df_nyc.plot(\n",
    "    linewidth=0.5,\n",
    "    color='White',\n",
    "    edgecolor='Black',\n",
    "    figsize=(15, 15),\n",
    "    alpha=0.75)\n",
    "\n",
    "errors_boro.plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    figsize=(15, 12),\n",
    "    c='red',\n",
    "    s=5,\n",
    "    alpha=0.25, ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "errors_boro.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Check cases where there was a reported borough code but not a detected one\n",
    "errors_boro2 = gdf [mask & ~gdf.BOROUGH.isnull() & gdf.GEO_BOROUGH.isnull()]\n",
    "errors_boro2 = pd.DataFrame(errors_boro2)\n",
    "len(errors_boro2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The plot reveals that most are border issues\n",
    "base = df_nyc.plot(\n",
    "    linewidth=0.5,\n",
    "    color='White',\n",
    "    edgecolor='Black',\n",
    "    figsize=(15, 15),\n",
    "    alpha=0.75)\n",
    "\n",
    "errors_boro2.plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    figsize=(15, 12),\n",
    "    c='red',\n",
    "    s=5,\n",
    "    alpha=0.25, ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "errors_boro2.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Discrepancy between detected and reported zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Code below lists cases where the borough is incorrectly coded based on lon/lat\n",
    "errors_zip = gdf [ mask & ~gdf.ZIPCODE.isnull() & ~gdf.GEO_ZIPCODE.isnull() & (gdf.GEO_ZIPCODE != gdf.ZIPCODE) ]\n",
    "errors_zip = pd.DataFrame(errors_zip)\n",
    "len(errors_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The plot reveals that most are border issues\n",
    "# but there are random errors spread throughout the city\n",
    "base = df_zip.plot(\n",
    "    linewidth=0.5,\n",
    "    color='White',\n",
    "    edgecolor='Black',\n",
    "    figsize=(15, 15),\n",
    "    alpha=0.75)\n",
    "\n",
    "errors_zip.plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    figsize=(15, 12),\n",
    "    c='red',\n",
    "    s=0.5,\n",
    "    alpha=0.25, ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "errors_zip.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Check cases where there was a reported ZIP code but not a detected one\n",
    "errors_zip2 = gdf [ mask & ~gdf.ZIPCODE.isnull() & gdf.GEO_ZIPCODE.isnull() ]\n",
    "errors_zip2 = pd.DataFrame(errors_zip2)\n",
    "len(errors_zip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "errors_zip2.pivot_table(\n",
    "    index='DATETIME',\n",
    "    values='UNIQUE_KEY',\n",
    "    aggfunc='count'\n",
    ").resample('1W').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# The plot reveals that most are border issues\n",
    "base = df_zip.plot(\n",
    "    linewidth=0.5,\n",
    "    color='White',\n",
    "    edgecolor='Black',\n",
    "    figsize=(15, 15),\n",
    "    alpha=0.75)\n",
    "\n",
    "errors_zip2.plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    figsize=(15, 12),\n",
    "    c='red',\n",
    "    s=2,\n",
    "    alpha=0.25, ax=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prepare Final DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "gdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Defines LON/LAT entries that are plausibly within NYC\n",
    "# We will use the mask to set to NULL all other lat/lon values\n",
    "# The analysis above indicates that the lon/lat that are within the mask\n",
    "# but \"do not match\" are actually, fine and most are border cases\n",
    "mask = (gdf.LATITUDE > 40) & (gdf.LATITUDE < 41) & (gdf.LONGITUDE < -72) & (gdf.LONGITUDE > -74.5)\n",
    "\n",
    "gdf.loc[~mask, 'LATITUDE'] = None\n",
    "gdf.loc[~mask, 'LONGITUDE'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Keeping the ZIP and BOROUGH from the original dataset \n",
    "# but changing its names, to distinguish from the detected\n",
    "# ones from the shapefiles\n",
    "\n",
    "gdf = gdf.rename(\n",
    "    {\n",
    "        'ZIPCODE': 'REPORTED_ZIPCODE',\n",
    "        'BOROUGH': 'REPORTED_BOROUGH',\n",
    "    },\n",
    "    axis='columns')\n",
    "\n",
    "gdf = gdf.rename(\n",
    "    {\n",
    "        'GEO_ZIPCODE': 'ZIPCODE',\n",
    "        'GEO_BOROUGH': 'BOROUGH',\n",
    "        'GEO_NEIGHBORHOOD': 'NEIGHBORHOOD'\n",
    "    },\n",
    "    axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Reorder the columns, dropping columns we do not need\n",
    "\n",
    "columns = [\n",
    "    'UNIQUE_KEY', 'DATETIME', 'LATITUDE', 'LONGITUDE', \n",
    "    'ZIPCODE', 'NEIGHBORHOOD', 'BOROUGH', \n",
    "    'PERSONS_INJURED', 'PERSONS_KILLED', \n",
    "    'PEDESTRIANS_INJURED', 'PEDESTRIANS_KILLED', \n",
    "    'CYCLISTS_INJURED', 'CYCLISTS_KILLED',\n",
    "    'MOTORISTS_INJURED', 'MOTORISTS_KILLED', \n",
    "    'ON_STREET_NAME', 'CROSS_STREET_NAME', 'OFF_STREET_NAME',\n",
    "    'REPORTED_ZIPCODE', 'REPORTED_BOROUGH'\n",
    "]\n",
    "\n",
    "gdf = gdf[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Sort by date\n",
    "gdf = gdf.sort_values('DATETIME')\n",
    "# Unique key is not unique\n",
    "gdf = gdf.groupby(gdf.UNIQUE_KEY).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "final_df = pd.DataFrame(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "base = df_nyc.plot(\n",
    "    linewidth=0.5,\n",
    "    color='White',\n",
    "    edgecolor='Black',\n",
    "    figsize=(15, 15),\n",
    "    alpha=0.75)\n",
    "\n",
    "scatterplot = final_df.plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    s=0.5,\n",
    "    alpha=0.02,\n",
    "    ax=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "final_df.to_csv('collisions.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# 4050018: Panos :-)\n",
    "final_df.query('UNIQUE_KEY  == \"4050018\"').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Contributing factor and Vehicle type\n",
    "\n",
    "* Examine the contents of Contributing factor and Vehicle type. Replace noisy values with UNSPECIFIED\n",
    "\n",
    "* Add a column \"NUM_VEHICLES\" to show the number of vehicles involved in the accident. Check that we do not have NaNs before a real value in CAUSE and in VEHICLE TYPE, and that the two columns (CAUSE AND VEHICLE) agree in the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "ct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "len(set(ct_df.index) - set(final_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "ct_df.CAUSE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# We replace noisy causes with UNSPECIFIED and not with NaN, as NaN is used to mean \"no vehicle\"\n",
    "import numpy as np\n",
    "ct_df.CAUSE = ct_df.CAUSE.replace(to_replace='1', value='UNSPECIFIED')\n",
    "ct_df.CAUSE = ct_df.CAUSE.replace(to_replace='80', value='UNSPECIFIED')\n",
    "ct_df.CAUSE = ct_df.CAUSE.replace(to_replace='ILLNES', value='ILLNESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "ct_df.VEHICLE_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "# Also consult https://data.ny.gov/api/assets/83055271-29A6-4ED4-9374-E159F30DB5AE\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='STATION WAGON/SPORT UTILITY VEHICLE', value='SPORT UTILITY / STATION WAGON')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SUBN', value='SPORT UTILITY / STATION WAGON')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SUBN/', value='SPORT UTILITY / STATION WAGON')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='WAGON', value='SPORT UTILITY / STATION WAGON')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BU', value='BUS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BS', value='BUS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SCHOOL BUS', value='BUS')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='P/SH', value='POWER SHOVEL')\n",
    "\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MULTI-WHEELED VEHICLE', value='LARGE COM VEH(6 OR MORE TIRES)')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SMALL', value='SMALL COM VEH(4 TIRES)')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SMALL COM VEH(4 TIRES) ', value='SMALL COM VEH(4 TIRES)')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='LIVER', value='LIVERY VEHICLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='LIMOU', value='LIVERY VEHICLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='LIMO', value='LIVERY VEHICLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='LIMO/', value='LIVERY VEHICLE')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FORKL', value='FORK LIFT')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FORK', value='FORK LIFT')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DUMP', value='DUMP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DUMPS', value='DUMP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DUMPT', value='DUMP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PUMP', value='DUMP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DP', value='DUMP TRUCK')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='GARBA', value='GARBAGE OR REFUSE')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='CONV', value='CONVERTIBLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='CONVE', value='CONVERTIBLE')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SP', value='SNOW PLOW')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='2 DR SEDAN', value='SEDAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='4 DR SEDAN', value='SEDAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SUDAN', value='SEDAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SE', value='SEDAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='4DS', value='SEDAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='4DSD', value='SEDAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='3-DOOR', value='SEDAN')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN CAMPER', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN T', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN F', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN A', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN W', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN/B', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAB', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VANG', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN C', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN/T', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VANETTE', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN`', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAV', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VN', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='VAN (', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='ENCLOSED BODY - REMOVABLE ENCLOSURE', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='ENCLOSED BODY - NONREMOVABLE ENCLOSURE', value='VAN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='ENCLO', value='VAN')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOTOR HOME', value='R/V')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOTORIZED HOME', value='R/V')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='RV', value='R/V')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='R/V C', value='R/V')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='RV/TR', value='R/V')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='H/WH', value='R/V') # house on wheels\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='AM', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='AMB', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='AMBU', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='AMBUL', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='ANBUL', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='AMABU', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='AMULA', value='AMBULANCE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='ABULA', value='AMBULANCE')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BICYC', value='BICYCLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BIKE', value='BICYCLE')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TANK', value='TANK TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TANKE', value='TANK TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TANKER', value='TANK TRUCK')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='OTHER', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNK', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNK,', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNKN', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNKNO', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNKNOWN', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNKOW', value='UNKNOWN')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='UNNKO', value='UNKNOWN')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SC', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SCOO', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SCOOT', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOTER', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOPD', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOPAD', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOPET', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOPEN', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='E BIK', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='E-BIK', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='E/BIK', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MINIBIKE', value='SCOOTER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOPED', value='SCOOTER')\n",
    "\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='US PO', value='USPS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='USPOS', value='USPS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='USPS2', value='USPS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='USPST', value='USPS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='U.S P', value='USPS')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='U.S.', value='USPS')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMME', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COM', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMM', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COM T', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COM.', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMMM', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMER', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMMM', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COM.', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMIX', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMPA', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMB', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMMU', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='COMM.', value='COMMERCIAL')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='C0MME', value='COMMERCIAL')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PASS', value='PASSENGER VEHICLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PAS', value='PASSENGER VEHICLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PASSE', value='PASSENGER VEHICLE')\n",
    "\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PEDIC', value='PEDICAB')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOW T', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOW', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOW TRUCK', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOWTR', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOW-T', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOWIN', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TOWER', value='TOW TRUCK / WRECKER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='G TOW', value='TOW TRUCK / WRECKER')\n",
    "\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TKP', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PK', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PICK', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PICK-', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PICKU', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='PICKUP WITH MOUNTED CAMPER', value='PICK-UP TRUCK')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FLAT BED', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FLAT RACK', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FLATB', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FLAT', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FLAT RACK', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FLATB', value='PICK-UP TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BACK', value='PICK-UP TRUCK')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FDNY', value='FIRE TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FIRE', value='FIRE TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FIRET', value='FIRE TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FD NY', value='FIRE TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FD TR', value='FIRE TRUCK')\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRACTOR TRUCK DIESEL', value='TRACTOR TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRACTOR TRUCK GASOLINE', value='TRACTOR TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRAC.', value='TRACTOR TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRAC', value='TRACTOR TRUCK')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRAIL', value='TRAILER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TR', value='TRAILER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRL', value='TRAILER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRAIL', value='TRAILER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRLR', value='TRAILER')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SEMI', value='TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SEMI-', value='TRUCK')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MACK', value='TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TK', value='TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRACT', value='TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRK', value='TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='TRACK', value='TRUCK')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='FB', value='BOX TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BOX T', value='BOX TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BOX', value='BOX TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='BOXTR', value='BOX TRUCK')\n",
    "\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOTOR', value='MOTORCYCLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MINICYCLE', value='MOTORCYCLE')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='MOTORBIKE', value='MOTORCYCLE')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DSNY', value='SANITATION')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DS', value='SANITATION')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='SANIT', value='SANITATION')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DELV', value='DELIVERY TRUCK')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='DEL', value='DELIVERY TRUCK')\n",
    "\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='RD/S', value='ROAD SWEEPER')\n",
    "ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace='OML/', value='LIVERY VEHICLE') # OMNIBUS LIVERY\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "import ngram\n",
    "import math\n",
    "freq = ct_df.VEHICLE_TYPE.value_counts()\n",
    "vtypes = set(ct_df.VEHICLE_TYPE.values)\n",
    "\n",
    "similarities = []\n",
    "for t1 in vtypes:\n",
    "    for t2 in vtypes:\n",
    "        s = ngram.NGram.compare(t1, t2, n=2)\n",
    "        f1 = freq.loc[t1]\n",
    "        f2 = freq.loc[t2]\n",
    "        r = abs(math.log10(f1/f2))\n",
    "        f = f1 + f2\n",
    "        if f2 > f1:\n",
    "            fr1 = f2\n",
    "            fr2 = f1\n",
    "            tp1 = t2\n",
    "            tp2 = t1\n",
    "        else:\n",
    "            fr1 = f1\n",
    "            fr2 = f2\n",
    "            tp1 = t1\n",
    "            tp2 = t2\n",
    "            \n",
    "        sim = {\n",
    "            \"t1\" : tp1,\n",
    "            \"f1\" : fr1,\n",
    "            \"t2\" : tp2,\n",
    "            \"f2\" : fr2,\n",
    "            \"sim\" : s,\n",
    "            \"ratio\" : r\n",
    "        }\n",
    "            \n",
    "        if t1>t2 and s>0.1 and f > 1000 and r>1.5 and min(f1,f2)<200:\n",
    "            similarities.append(sim)\n",
    "\n",
    "pd.DataFrame(similarities).sort_values('sim', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "freq = ct_df.VEHICLE_TYPE.value_counts()\n",
    "\n",
    "replace = freq [ freq <= 10]\n",
    "for r in replace.index.values:\n",
    "    ct_df.VEHICLE_TYPE = ct_df.VEHICLE_TYPE.replace(to_replace=r, value='UNKNOWN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "ct_df.to_csv('collisions-causes.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Count the number of vehicles involved in every accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "# Group by ct_df and count the number of vehicles involved in every accident\n",
    "# Also find the max number of the vehicle columns in each accident\n",
    "# This will help determine if there is a gap between values\n",
    "number_of_vehicles = ct_df.groupby(ct_df.index).agg({ 'VEHICLE': ['count', np.max] })\n",
    "# Drop top level in columns axis in order to select column more easily\n",
    "number_of_vehicles.columns = number_of_vehicles.columns.droplevel(0)\n",
    "# Filter all columns that do not have the count equal to the max number\n",
    "# This means that there was a gap between the values in the original dataset\n",
    "number_of_vehicles = number_of_vehicles.query('count == amax')['count']\n",
    "# Perform a join with the collisions dataset to add the NUM_VEHICLES column\n",
    "final_df = final_df.join(number_of_vehicles)\n",
    "final_df = final_df.rename(columns={'count': 'NUM_VEHICLES'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Store data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "def addToTable(engine, db_name, table, data, useIndex=False):\n",
    "    batchsize = 50000\n",
    "    batches = len(data) // batchsize + 1\n",
    "\n",
    "    t = tqdm(range(batches))\n",
    "\n",
    "    if db_name == \"\":\n",
    "        for i in t:\n",
    "            # print(\"Batch:\",i)\n",
    "            # continue # Cannot execute this on Travis\n",
    "            start = batchsize * i\n",
    "            end = batchsize * (i+1)\n",
    "            data[start:end].to_sql(\n",
    "                name = table,\n",
    "                con = engine,\n",
    "                if_exists = 'append',\n",
    "                index = useIndex, \n",
    "                chunksize = 1000)\n",
    "    else:\n",
    "        for i in t:\n",
    "            # print(\"Batch:\",i)\n",
    "            # continue # Cannot execute this on Travis\n",
    "            start = batchsize * i\n",
    "            end = batchsize * (i+1)\n",
    "            data[start:end].to_sql(\n",
    "                name = table, \n",
    "                schema = db_name, \n",
    "                con = engine,\n",
    "                if_exists = 'append',\n",
    "                index = useIndex, \n",
    "                chunksize = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# Create url to connect to database\n",
    "conn_string = 'mysql://{user}:{password}@{host}/?charset={encoding}'.format(\n",
    "host = 'localhost', \n",
    "user = 'root',\n",
    "password = 'root',\n",
    "encoding = 'utf8mb4')\n",
    "# Create engine and connect to database MySQL\n",
    "engine = create_engine(conn_string)\n",
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "db_name = 'ny_collisions'\n",
    "charset = 'utf8mb4'\n",
    "\n",
    "sql = f'DROP DATABASE IF EXISTS {db_name}'\n",
    "engine.execute(sql)\n",
    "\n",
    "sql = f'CREATE DATABASE IF NOT EXISTS {db_name} DEFAULT CHARACTER SET {charset}'\n",
    "engine.execute(sql)\n",
    "\n",
    "sql = f'USE {db_name}'\n",
    "engine.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "collisionTable = \"\"\"\n",
    "CREATE TABLE Collisions (\n",
    "    UNIQUE_KEY varchar(7) PRIMARY KEY,\n",
    "    DATETIME Datetime,\n",
    "    LATITUDE float,\n",
    "    LONGITUDE float,\n",
    "    ON_STREET_NAME tinytext,\n",
    "    CROSS_STREET_NAME tinytext,\n",
    "    OFF_STREET_NAME tinytext,\n",
    "    ZIPCODE varchar(5),\n",
    "    REPORTED_ZIPCODE varchar(5),\n",
    "    NEIGHBORHOOD tinytext,\n",
    "    BOROUGH tinytext,\n",
    "    REPORTED_BOROUGH tinytext,\n",
    "    PERSONS_INJURED int(3),\n",
    "    PERSONS_KILLED int(3),\n",
    "    PEDESTRIANS_INJURED int(3),\n",
    "    PEDESTRIANS_KILLED int(3),\n",
    "    MOTORISTS_INJURED int(3),\n",
    "    MOTORISTS_KILLED int(3),\n",
    "    CYCLISTS_INJURED int(3),\n",
    "    CYCLISTS_KILLED int(3),\n",
    "    NUM_VEHICLES int(1)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "engine.execute(collisionTable)\n",
    "\n",
    "addToTable(con, db_name, table='Collisions', data=final_df, useIndex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "typesCausesTable = \"\"\"\n",
    "CREATE TABLE VehiclesInvolved (\n",
    "    UNIQUE_KEY varchar(7) NOT NULL,\n",
    "    VEHICLE_TYPE tinytext,\n",
    "    CAUSE tinytext,\n",
    "    VEHICLE int,\n",
    "    FOREIGN KEY (UNIQUE_KEY) REFERENCES Collisions(UNIQUE_KEY),\n",
    "    INDEX (UNIQUE_KEY)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "engine.execute(typesCausesTable)\n",
    "\n",
    "addToTable(con, db_name, table='VehiclesInvolved', data=ct_df, useIndex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "!rm ny_collisions.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "db_name = 'ny_collisions.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "collisionTable = \"\"\"\n",
    "CREATE TABLE Collisions (\n",
    "    UNIQUE_KEY varchar(7) PRIMARY KEY,\n",
    "    DATETIME Datetime,\n",
    "    LATITUDE float,\n",
    "    LONGITUDE float,\n",
    "    ON_STREET_NAME text,\n",
    "    CROSS_STREET_NAME text,\n",
    "    OFF_STREET_NAME text,\n",
    "    ZIPCODE varchar(5),\n",
    "    REPORTED_ZIPCODE varchar(5),\n",
    "    NEIGHBORHOOD text,\n",
    "    BOROUGH text,\n",
    "    REPORTED_BOROUGH text,\n",
    "    PERSONS_INJURED int,\n",
    "    PERSONS_KILLED int,\n",
    "    PEDESTRIANS_INJURED int,\n",
    "    PEDESTRIANS_KILLED int,\n",
    "    MOTORISTS_INJURED int,\n",
    "    MOTORISTS_KILLED int,\n",
    "    CYCLISTS_INJURED int,\n",
    "    CYCLISTS_KILLED int,\n",
    "    NUM_VEHICLES int\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute('DROP TABLE IF EXISTS Collisions')\n",
    "cursor.execute(collisionTable)\n",
    "\n",
    "addToTable(con, db_name, table='Collisions', data=final_df, useIndex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "typesCausesTable = \"\"\"\n",
    "CREATE TABLE VehiclesInvolved (\n",
    "    UNIQUE_KEY varchar(7) NOT NULL,\n",
    "    VEHICLE_TYPE text,\n",
    "    CAUSE text,\n",
    "    VEHICLE int\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute('DROP TABLE IF EXISTS VehiclesInvolved')\n",
    "cursor.execute(typesCausesTable)\n",
    "cursor.execute('CREATE INDEX unique_key ON VehiclesInvolved(UNIQUE_KEY)')\n",
    "addToTable(con, db_name, table='VehiclesInvolved', data=ct_df, useIndex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
   ],
   "source": [
    "final_df.to_csv('collisions.csv.gz', index=True, compression='gzip')\n",
    "ct_df.to_csv('types_causes.csv.gz', index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "final_df.index.name = final_df.index.name.lower()\n",
    "final_df.columns = map(str.lower, final_df.columns)\n",
    "ct_df.index.name = ct_df.index.name.lower()\n",
    "ct_df.columns = map(str.lower, ct_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "conn_string = 'postgresql://{user}:{password}@{host}:{port}/ny_collisions'.format(\n",
    "    host = 'localhost', \n",
    "    user = 'postgres',\n",
    "    password = '123456789abc',\n",
    "    port = '5432'\n",
    ")\n",
    "\n",
    "engine = create_engine(conn_string)\n",
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "engine.execute('DROP TABLE IF EXISTS vehicles_involved')\n",
    "engine.execute('DROP TABLE IF EXISTS collisions')\n",
    "\n",
    "collisionTable = \"\"\"\n",
    "CREATE TABLE collisions (\n",
    "    unique_key varchar(7) PRIMARY KEY,\n",
    "    datetime timestamp,\n",
    "    latitude float,\n",
    "    longitude float,\n",
    "    on_street_name text,\n",
    "    cross_street_name text,\n",
    "    off_street_name text,\n",
    "    zipcode varchar(5),\n",
    "    reported_zipcode varchar(5),\n",
    "    neighborhood text,\n",
    "    borough text,\n",
    "    reported_borough text,\n",
    "    persons_injured smallint,\n",
    "    persons_killed smallint,\n",
    "    pedestrians_injured smallint,\n",
    "    pedestrians_killed smallint,\n",
    "    motorists_injured smallint,\n",
    "    motorists_killed smallint,\n",
    "    cyclists_injured smallint,\n",
    "    cyclists_killed smallint,\n",
    "    num_vehicles smallint\n",
    ");\n",
    "\"\"\"\n",
    "engine.execute(collisionTable)\n",
    "\n",
    "addToTable(con, \"\", table='collisions', data=final_df, useIndex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "typesCausesTable = \"\"\"\n",
    "CREATE TABLE vehicles_involved (\n",
    "    unique_key varchar(7) NOT NULL,\n",
    "    vehicle_type text,\n",
    "    cause text,\n",
    "    vehicle smallint,\n",
    "    FOREIGN KEY (unique_key) REFERENCES collisions(unique_key)\n",
    ");\n",
    "\"\"\"\n",
    "engine.execute(typesCausesTable)\n",
    "engine.execute('CREATE INDEX unique_key ON vehicles_involved(unique_key)')\n",
    "\n",
    "addToTable(con, \"\", table='vehicles_involved', data=ct_df, useIndex=True)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "t_end = datetime.now()\n",
    "\n",
    "t_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing\n",
    "This part of the script is used to validate the integrity \n",
    "of the incoming dataset and point out inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def init():\n",
    "    # configure logging\n",
    "    logger = logging.getLogger()\n",
    "    handler = logging.FileHandler('.log','w')\n",
    "    formatter = logging.Formatter('[%(asctime)s] %(levelname)-8s %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def readCSV():\n",
    "    logging.info('Reading CSV...')\n",
    "    return pd.read_csv('accidents.csv', dtype = 'object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def checkColumns(data):\n",
    "    errors = False\n",
    "    logging.info('***Checking if any columns have benn removed or added to the dataset.***')\n",
    "    # Check if all columns needed are in the dataset\n",
    "    VALUES = [\n",
    "        'DATE','TIME','BOROUGH','ZIP CODE','LATITUDE','LONGITUDE','LOCATION',\n",
    "        'ON STREET NAME','CROSS STREET NAME','OFF STREET NAME','NUMBER OF PERSONS INJURED',\n",
    "        'NUMBER OF PERSONS KILLED','NUMBER OF PEDESTRIANS INJURED','NUMBER OF PEDESTRIANS KILLED',\n",
    "        'NUMBER OF CYCLIST INJURED','NUMBER OF CYCLIST KILLED','NUMBER OF MOTORIST INJURED',\n",
    "        'NUMBER OF MOTORIST KILLED','CONTRIBUTING FACTOR VEHICLE 1','CONTRIBUTING FACTOR VEHICLE 2',\n",
    "        'CONTRIBUTING FACTOR VEHICLE 3','CONTRIBUTING FACTOR VEHICLE 4','CONTRIBUTING FACTOR VEHICLE 5',\n",
    "        'UNIQUE KEY','VEHICLE TYPE CODE 10','VEHICLE TYPE CODE 2','VEHICLE TYPE CODE 3',\n",
    "        'VEHICLE TYPE CODE 4','VEHICLE TYPE CODE 5'\n",
    "    ]\n",
    "    sortedInput = list(data.columns)\n",
    "    sortedInput.sort()\n",
    "    VALUES.sort()\n",
    "    if (VALUES != sortedInput):\n",
    "        added = set(sortedInput).difference(VALUES)\n",
    "        removed = set(VALUES).difference(sortedInput)\n",
    "        logging.error('Inconsistency found...')\n",
    "        if (len(added) != 0):\n",
    "            logging.error('New columns added to the dataset: ' + str(added) + '.')\n",
    "        if (len(removed) != 0):\n",
    "            logging.error('Columns removed from the dataset: ' + str(removed) + '.')\n",
    "        errors = True\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def main():\n",
    "    errors = False\n",
    "    init()\n",
    "    logging.info('Running Tests...')\n",
    "    df = readCSV()\n",
    "    checkColumns(df)\n",
    "\n",
    "# in order to execute the script you just need to call the main function\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}