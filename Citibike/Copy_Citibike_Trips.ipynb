{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMfZ6aaS9UNSZyLceHyAlpm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis-org/datasets/blob/main/Citibike/Copy_Citibike_Trips.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "5Gf6MX5bWLZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from google.cloud import storage\n",
        "\n",
        "# Replace these variables with your details\n",
        "gcs_bucket_name = 'citibike-archive'\n",
        "gcs_folder = 'tripdata'\n",
        "\n",
        "# Initialize the Google Cloud Storage client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(gcs_bucket_name)\n",
        "\n",
        "# Function to download a file from a URL\n",
        "def download_file(url, local_filename):\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    return local_filename\n",
        "\n",
        "# Function to upload a file to Google Cloud Storage\n",
        "def upload_to_gcs(local_file, bucket_name, destination_blob_name):\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(local_file)\n",
        "    print(f\"File {local_file} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "  # Function to normalize column names\n",
        "def normalize_columns(df):\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "    return df"
      ],
      "metadata": {
        "id": "HfFmdKQZZECG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def load_csv_from_gcs(project_id, dataset_id, table_id, gcs_path):\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,  # Adjust if your CSV has a header row\n",
        "        autodetect=True,\n",
        "    )\n",
        "\n",
        "    load_job = client.load_table_from_uri(\n",
        "        gcs_path, table_ref, job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"Starting job {load_job.job_id}\")\n",
        "\n",
        "    load_job.result()  # Waits for the job to complete.\n",
        "\n",
        "    print(f\"Job finished. Loaded {load_job.output_rows} rows into {dataset_id}:{table_id}\")\n"
      ],
      "metadata": {
        "id": "YBSKUcNKwzDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the standardized schema\n",
        "standard_schema = {\n",
        "    \"tripduration\": \"int64\",\n",
        "    \"starttime\": \"datetime64[ns]\",\n",
        "    \"stoptime\": \"datetime64[ns]\",\n",
        "    \"start_station_id\": \"str\",\n",
        "    \"start_station_name\": \"str\",\n",
        "    \"start_station_latitude\": \"float64\",\n",
        "    \"start_station_longitude\": \"float64\",\n",
        "    \"end_station_id\": \"str\",\n",
        "    \"end_station_name\": \"str\",\n",
        "    \"end_station_latitude\": \"float64\",\n",
        "    \"end_station_longitude\": \"float64\",\n",
        "    \"bikeid\": \"str\",\n",
        "    \"usertype\": \"str\",\n",
        "    \"birth_year\": \"int64\",\n",
        "    \"gender\": \"int64\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "zmrPuo3WFv2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2013"
      ],
      "metadata": {
        "id": "5XwZk4xSZbJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_parquet(folder_path, bucket_name, gcs_folder):\n",
        "# Iterate over all files in the folder\n",
        "  for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".csv\"):\n",
        "        local_file = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(local_file, low_memory=False, dtype='str')\n",
        "\n",
        "        df = normalize_columns(df)\n",
        "\n",
        "        df.replace(r\"NULL\", pd.NA, inplace=True)\n",
        "        df.replace(r\"\\N\", pd.NA, inplace=True)\n",
        "        df.replace(r\"\\.0$\", r\"\", regex=True, inplace=True)\n",
        "\n",
        "\n",
        "        for column, dtype in standard_schema.items():\n",
        "            if column in df.columns:\n",
        "                # print(f\"Converting column '{column}' to {dtype}...\")\n",
        "                df[column] = df[column].astype(dtype, errors='ignore')\n",
        "\n",
        "            else:\n",
        "                print(f\"Column '{column}' not found in CSV file.\")\n",
        "                df[column] = None\n",
        "\n",
        "        parquet_file = local_file.replace(\".csv\", \".parquet\")\n",
        "        df.to_parquet(parquet_file, index=False)\n",
        "\n",
        "        # Upload Parquet file to GCS\n",
        "        gcs_destination = f\"{gcs_folder}/{os.path.basename(parquet_file)}\"\n",
        "        upload_to_gcs(parquet_file, bucket_name, gcs_destination)\n",
        "\n",
        "        # Remove local Parquet file after upload\n",
        "        os.remove(parquet_file)\n"
      ],
      "metadata": {
        "id": "7ADo9lUFR_NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a BigQuery table on top of Parquet files in GCS\n",
        "def create_bigquery_table_from_gcs_parquet(project_id, dataset_id, table_id, gcs_folder):\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    external_config = bigquery.ExternalConfig(\"PARQUET\")\n",
        "    external_config.source_uris = [f\"gs://{bucket_name}/{gcs_folder}/*.parquet\"]\n",
        "    table = bigquery.Table(table_ref)\n",
        "    table.external_data_configuration = external_config\n",
        "\n",
        "    table = client.create_table(table)\n",
        "    print(f\"Table {table_id} created in dataset {dataset_id}.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GjVZbTAAIu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://s3.amazonaws.com/tripdata/2013-citibike-tripdata.zip -o 2013-citibike-tripdata.zip\n",
        "!unzip 2013-citibike-tripdata.zip\n",
        "!rm -rf __MACOSX*\n",
        "!rm 2013-citibike-tripdata.zip\n",
        "\n",
        "# Remove the files under the folder structure\n",
        "# Seems that the files in the root folder use quotes and they are also one file per month\n",
        "!rm -rf 2013-citibike-tripdata/*_*"
      ],
      "metadata": {
        "id": "VBSNxjxDZco6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the bucket name and dataset/table details\n",
        "bucket_name = \"citibike-archive\"\n",
        "dataset_id = \"citibike\"\n",
        "table_id = \"trips\"\n",
        "create_bigquery_table_from_gcs_parquet(\"nyu-datasets\", dataset_id, table_id, \"tripdata\")\n"
      ],
      "metadata": {
        "id": "0ev4lHvRSjqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the folder containing the CSV files\n",
        "folder_path = \"2013-citibike-tripdata\"\n",
        "\n",
        "convert_to_parquet(folder_path, bucket_name, gcs_folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "UtJtZROnbI_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://s3.amazonaws.com/tripdata/2014-citibike-tripdata.zip -o 2014-citibike-tripdata.zip\n",
        "!unzip 2014-citibike-tripdata.zip\n",
        "\n",
        "!mv 2014-citibike-tripdata/1_January/201401-citibike-tripdata_1.csv 2014-citibike-tripdata/201401-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/2_February/201402-citibike-tripdata_1.csv 2014-citibike-tripdata/201402-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/3_March/201403-citibike-tripdata_1.csv 2014-citibike-tripdata/201403-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/4_April/201404-citibike-tripdata_1.csv 2014-citibike-tripdata/201404-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/5_May/201405-citibike-tripdata_1.csv 2014-citibike-tripdata/201405-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/6_June/201406-citibike-tripdata_1.csv 2014-citibike-tripdata/201406-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/7_July/201407-citibike-tripdata_1.csv 2014-citibike-tripdata/201407-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/8_August/201408-citibike-tripdata_1.csv 2014-citibike-tripdata/201408-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/9_September/201409-citibike-tripdata_1.csv 2014-citibike-tripdata/201409-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/10_October/201410-citibike-tripdata_1.csv 2014-citibike-tripdata/201410-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/11_November/201411-citibike-tripdata_1.csv 2014-citibike-tripdata/201411-citibike-tripdata.csv\n",
        "!mv 2014-citibike-tripdata/12_December/201412-citibike-tripdata_1.csv 2014-citibike-tripdata/201412-citibike-tripdata.csv\n",
        "\n",
        "\n",
        "!rm -rf __MACOSX*\n",
        "!rm 2014-citibike-tripdata.zip\n",
        "\n",
        "# Remove the files under the folder structure\n",
        "# Seems that the files in the root folder use quotes and they are also one file per month\n",
        "!rm -rf 2014-citibike-tripdata/*_*"
      ],
      "metadata": {
        "id": "7ZvZUDfRTRRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cyfRWvvmTimZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39QcGv0FPWRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the CSV files\n",
        "folder_path = \"2014-citibike-tripdata\"\n",
        "\n",
        "convert_to_parquet(folder_path, bucket_name, gcs_folder)\n"
      ],
      "metadata": {
        "id": "-PlTCQw0bVvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://s3.amazonaws.com/tripdata/2015-citibike-tripdata.zip -o 2015-citibike-tripdata.zip\n",
        "!unzip 2015-citibike-tripdata.zip\n"
      ],
      "metadata": {
        "id": "DEM76PxGXTAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mv 2015-citibike-tripdata/1_January/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/2_February/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/3_March/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/4_April/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/5_May/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/6_June/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/7_July/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/8_August/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/9_September/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/10_October/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/11_November/* 2015-citibike-tripdata/\n",
        "!mv 2015-citibike-tripdata/12_December/* 2015-citibike-tripdata/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GEI1WN3VXVp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the CSV files\n",
        "folder_path = \"2015-citibike-tripdata\"\n",
        "\n",
        "convert_to_parquet(folder_path, bucket_name, gcs_folder)\n"
      ],
      "metadata": {
        "id": "-AVnDz4vaYST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d',' -f14 2015-citibike-tripdata/*.csv | sort | uniq"
      ],
      "metadata": {
        "id": "3JbTZ6O8bMaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.birth_year.unique()"
      ],
      "metadata": {
        "id": "w6RSP8YHbmR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!rm -rf __MACOSX*\n",
        "!rm 2015-citibike-tripdata.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "C5vJdZQVYXu-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}