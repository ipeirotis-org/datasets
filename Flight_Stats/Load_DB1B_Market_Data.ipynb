{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis-org/datasets/blob/main/Flight_Stats/Load_DB1B_Market_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS--RhVEa3rJ"
      },
      "source": [
        "# DB1B Market Data Pipeline\n",
        "\n",
        "## Overview\n",
        "This notebook loads the **DB1B Origin & Destination Survey** from the Bureau of Transportation Statistics (BTS) into BigQuery.\n",
        "\n",
        "**What is DB1B?**\n",
        "- A 10% sample of all airline tickets sold in the US\n",
        "- Contains origin, destination, fare, carrier, and routing information\n",
        "- Published quarterly by the US Department of Transportation\n",
        "- Critical for analyzing airline pricing, competition, and market structure\n",
        "\n",
        "**Pipeline Architecture:**\n",
        "1. **Download**: Fetch quarterly ZIP files from BTS\n",
        "2. **Stage**: Upload raw CSVs to Google Cloud Storage\n",
        "3. **Validate**: Check schema consistency across years\n",
        "4. **Load**: Use BigQuery load jobs to import from GCS\n",
        "\n",
        "**Key Features:**\n",
        "- Resume capability (skips already-loaded quarters)\n",
        "- Schema evolution tracking\n",
        "- Data quality validation\n",
        "- Partitioned and clustered BigQuery table for query performance\n",
        "\n",
        "**Data Source:** https://www.transtats.bts.gov/DatabaseInfo.asp?QO_VQ=EFI&Yv0x=D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx1Kuftra3rK"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdoR47rAa3rL"
      },
      "outputs": [],
      "source": [
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    # GCP Settings\n",
        "    'PROJECT_ID': 'nyu-datasets',\n",
        "    'DATASET_ID': 'flights',\n",
        "    'TABLE_NAME': 'raw_db1b_market',\n",
        "    'GCS_BUCKET': 'bts_datasets',  # Bucket for staging CSV files\n",
        "    'GCS_PREFIX': 'db1b_market/',           # Folder within bucket\n",
        "\n",
        "    # Data Range\n",
        "    'YEARS': list(range(2025, 2026)),\n",
        "    'QUARTERS': [1, 2],\n",
        "\n",
        "    # Processing\n",
        "    'MAX_RETRIES': 3,\n",
        "    'RETRY_DELAY': 5,  # seconds\n",
        "    'SAMPLE_RATE': 10,  # DB1B is a 10% sample\n",
        "\n",
        "    # BTS URL Template\n",
        "    'BASE_URL': 'https://transtats.bts.gov/PREZIP/Origin_and_Destination_Survey_DB1BMarket_{}_{}.zip'\n",
        "}\n",
        "\n",
        "print(f\"Configuration loaded.\")\n",
        "print(f\"Will process {len(CONFIG['YEARS'])} years × {len(CONFIG['QUARTERS'])} quarters = {len(CONFIG['YEARS']) * len(CONFIG['QUARTERS'])} files\")\n",
        "print(f\"Target: {CONFIG['PROJECT_ID']}.{CONFIG['DATASET_ID']}.{CONFIG['TABLE_NAME']}\")\n",
        "print(f\"Staging: gs://{CONFIG['GCS_BUCKET']}/{CONFIG['GCS_PREFIX']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqJjvlsea3rO"
      },
      "source": [
        "## Setup and Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3FkpjbJa3rO"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "import gc\n",
        "import time\n",
        "from typing import List, Dict, Optional, Tuple, Set\n",
        "from datetime import datetime\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=CONFIG['PROJECT_ID'])\n",
        "storage_client = storage.Client(project=CONFIG['PROJECT_ID'])\n",
        "\n",
        "print(\"✓ Authentication successful\")\n",
        "print(f\"✓ BigQuery client initialized for project: {CONFIG['PROJECT_ID']}\")\n",
        "print(f\"✓ Storage client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3437AUYa3rP"
      },
      "source": [
        "## Column Metadata\n",
        "\n",
        "Official column descriptions from the DOT data dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_inVYWMla3rQ"
      },
      "outputs": [],
      "source": [
        "COLUMN_DESCRIPTIONS = {\n",
        "    \"ItinID\": \"Itinerary ID. Identification number assigned to identify an itinerary. Foreign key to DB1BTicket.\",\n",
        "    \"MktID\": \"Market ID. Identification number assigned to identify a market. Foreign key to DB1BMarket.\",\n",
        "    \"MktCoupons\": \"Number of Coupons in the Market. The number of flight segments in the market.\",\n",
        "    \"Year\": \"Year of the survey.\",\n",
        "    \"Quarter\": \"Quarter of the survey (1-4).\",\n",
        "    \"Origin\": \"Origin Airport Code (e.g., JFK, ORD).\",\n",
        "    \"OriginAirportID\": \"Origin Airport ID. A unique numeric code assigned by US DOT to the origin airport.\",\n",
        "    \"OriginAirportSeqID\": \"Origin Airport Sequence ID.\",\n",
        "    \"OriginCityMarketID\": \"Origin City Market ID. Use this field to consolidate airports serving the same city market.\",\n",
        "    \"OriginCountry\": \"Origin Country Code.\",\n",
        "    \"OriginStateFips\": \"Origin State FIPS Code.\",\n",
        "    \"OriginState\": \"Origin State Code.\",\n",
        "    \"OriginStateName\": \"Origin State Name.\",\n",
        "    \"OriginWac\": \"Origin World Area Code (WAC). Geographic area code for the origin.\",\n",
        "    \"Dest\": \"Destination Airport Code (e.g., LAX, SFO).\",\n",
        "    \"DestAirportID\": \"Destination Airport ID. A unique numeric code assigned by US DOT to the destination airport.\",\n",
        "    \"DestAirportSeqID\": \"Destination Airport Sequence ID.\",\n",
        "    \"DestCityMarketID\": \"Destination City Market ID. Use this field to consolidate airports serving the same city market.\",\n",
        "    \"DestCountry\": \"Destination Country Code.\",\n",
        "    \"DestStateFips\": \"Destination State FIPS Code.\",\n",
        "    \"DestState\": \"Destination State Code.\",\n",
        "    \"DestStateName\": \"Destination State Name.\",\n",
        "    \"DestWac\": \"Destination World Area Code (WAC). Geographic area code for the destination.\",\n",
        "    \"AirportGroup\": \"Airport Group. Sequence of airports in the market.\",\n",
        "    \"WacGroup\": \"World Area Code Group. Sequence of WACs in the market.\",\n",
        "    \"TkCarrier\": \"Ticketing Carrier. The airline that sold the ticket.\",\n",
        "    \"TkCarrierChange\": \"Ticketing Carrier Change Indicator. 1 if the ticketing carrier changes within the market; 0 otherwise.\",\n",
        "    \"OpCarrier\": \"Operating Carrier. The airline that actually operated the flight.\",\n",
        "    \"OpCarrierChange\": \"Operating Carrier Change Indicator. 1 if the operating carrier changes within the market; 0 otherwise.\",\n",
        "    \"RPCarrier\": \"Reporting Carrier. The airline that submitted the data to DOT.\",\n",
        "    \"TkCarrierGroup\": \"Ticketing Carrier Group. Sequence of ticketing carriers.\",\n",
        "    \"OpCarrierGroup\": \"Operating Carrier Group. Sequence of operating carriers.\",\n",
        "    \"Passengers\": \"Number of Passengers. 10% sample count (multiply by 10 for total estimate).\",\n",
        "    \"MktFare\": \"Market Fare. The prorated fare for this specific market (one-way portion of the trip).\",\n",
        "    \"BulkFare\": \"Bulk Fare. Fare paid for the entire itinerary (round-trip or one-way).\",\n",
        "    \"MktDistance\": \"Market Distance. Non-stop distance between origin and destination.\",\n",
        "    \"MktMilesFlown\": \"Market Miles Flown. Actual miles flown for this market (may differ from non-stop distance).\",\n",
        "    \"NonStopMiles\": \"Non-Stop Miles. Great circle distance between origin and destination.\",\n",
        "    \"ItinGeoType\": \"Itinerary Geography Type. 1=Domestic, 2=International.\",\n",
        "    \"MktGeoType\": \"Market Geography Type. 1=Domestic, 2=International.\",\n",
        "    \"MktDistanceGroup\": \"Market Distance Group. Categorical grouping of market distances.\",\n",
        "    \"Unnamed: 41\": \"Unknown column that appears in data from 2020 onwards. Appears to be empty/null. Likely a data artifact.\"\n",
        "}\n",
        "\n",
        "# Define expected data types for better memory efficiency\n",
        "COLUMN_DTYPES = {\n",
        "    'ItinID': 'int64',\n",
        "    'MktID': 'int32',\n",
        "    'MktCoupons': 'int8',\n",
        "    'Year': 'int16',\n",
        "    'Quarter': 'int8',\n",
        "    'Origin': 'category',\n",
        "    'OriginAirportID': 'int32',\n",
        "    'OriginAirportSeqID': 'int32',\n",
        "    'OriginCityMarketID': 'int32',\n",
        "    'OriginCountry': 'category',\n",
        "    'OriginStateFips': 'category',\n",
        "    'OriginState': 'category',\n",
        "    'OriginStateName': 'category',\n",
        "    'OriginWac': 'int16',\n",
        "    'Dest': 'category',\n",
        "    'DestAirportID': 'int32',\n",
        "    'DestAirportSeqID': 'int32',\n",
        "    'DestCityMarketID': 'int32',\n",
        "    'DestCountry': 'category',\n",
        "    'DestStateFips': 'category',\n",
        "    'DestState': 'category',\n",
        "    'DestStateName': 'category',\n",
        "    'DestWac': 'int16',\n",
        "    'TkCarrier': 'category',\n",
        "    'TkCarrierChange': 'int8',\n",
        "    'OpCarrier': 'category',\n",
        "    'OpCarrierChange': 'int8',\n",
        "    'RPCarrier': 'category',\n",
        "    'Passengers': 'int32',\n",
        "    'MktFare': 'float32',\n",
        "    'BulkFare': 'float32',\n",
        "    'MktDistance': 'int16',\n",
        "    'MktMilesFlown': 'int16',\n",
        "    'NonStopMiles': 'int16',\n",
        "    'ItinGeoType': 'int8',\n",
        "    'MktGeoType': 'int8',\n",
        "    'MktDistanceGroup': 'int8'\n",
        "}\n",
        "\n",
        "print(f\"✓ Loaded metadata for {len(COLUMN_DESCRIPTIONS)} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpIy17kva3rR"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpuupxSDa3rR"
      },
      "outputs": [],
      "source": [
        "def get_gcs_path(year: int, quarter: int) -> str:\n",
        "    \"\"\"Generate GCS path for a given year/quarter.\"\"\"\n",
        "    return f\"{CONFIG['GCS_PREFIX']}{year}_Q{quarter}.csv\"\n",
        "\n",
        "def get_table_id() -> str:\n",
        "    \"\"\"Get full BigQuery table ID.\"\"\"\n",
        "    return f\"{CONFIG['PROJECT_ID']}.{CONFIG['DATASET_ID']}.{CONFIG['TABLE_NAME']}\"\n",
        "\n",
        "def file_exists_in_gcs(year: int, quarter: int) -> bool:\n",
        "    \"\"\"Check if a file already exists in GCS.\"\"\"\n",
        "    bucket = storage_client.bucket(CONFIG['GCS_BUCKET'])\n",
        "    blob = bucket.blob(get_gcs_path(year, quarter))\n",
        "    return blob.exists()\n",
        "\n",
        "def get_loaded_quarters() -> Set[Tuple[int, int]]:\n",
        "    \"\"\"Query BigQuery to find which year/quarter combinations are already loaded.\"\"\"\n",
        "    table_id = get_table_id()\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        SELECT DISTINCT Year, Quarter\n",
        "        FROM `{table_id}`\n",
        "        ORDER BY Year, Quarter\n",
        "        \"\"\"\n",
        "        results = bq_client.query(query).result()\n",
        "        loaded = {(row.Year, row.Quarter) for row in results}\n",
        "        return loaded\n",
        "    except Exception as e:\n",
        "        # Table doesn't exist yet\n",
        "        return set()\n",
        "\n",
        "def validate_dataframe(df: pd.DataFrame, year: int, quarter: int) -> List[str]:\n",
        "    \"\"\"Validate data quality of a dataframe.\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check row count\n",
        "    if len(df) < 10000:\n",
        "        issues.append(f\"Suspiciously low row count: {len(df)}\")\n",
        "    elif len(df) > 10_000_000:\n",
        "        issues.append(f\"Suspiciously high row count: {len(df)}\")\n",
        "\n",
        "    # Check critical columns aren't all null\n",
        "    critical_cols = ['Origin', 'Dest', 'Passengers', 'MktFare']\n",
        "    for col in critical_cols:\n",
        "        if col in df.columns and df[col].isna().all():\n",
        "            issues.append(f\"Critical column '{col}' is entirely NULL\")\n",
        "\n",
        "    # Check Year/Quarter match expected values\n",
        "    if 'Year' in df.columns and not df['Year'].isna().all():\n",
        "        unique_years = df['Year'].unique()\n",
        "        if len(unique_years) != 1 or unique_years[0] != year:\n",
        "            issues.append(f\"Year mismatch: expected {year}, found {unique_years}\")\n",
        "\n",
        "    if 'Quarter' in df.columns and not df['Quarter'].isna().all():\n",
        "        unique_quarters = df['Quarter'].unique()\n",
        "        if len(unique_quarters) != 1 or unique_quarters[0] != quarter:\n",
        "            issues.append(f\"Quarter mismatch: expected {quarter}, found {unique_quarters}\")\n",
        "\n",
        "    # Check passengers are positive\n",
        "    if 'Passengers' in df.columns:\n",
        "        negative_passengers = (df['Passengers'] < 0).sum()\n",
        "        if negative_passengers > 0:\n",
        "            issues.append(f\"Found {negative_passengers} rows with negative passengers\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "print(\"✓ Utility functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cps4fPzKa3rS"
      },
      "source": [
        "## Download Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndDLB_FOa3rS"
      },
      "outputs": [],
      "source": [
        "def download_and_upload_to_gcs(year: int, quarter: int, force: bool = False) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Download a DB1B file from BTS and upload to GCS.\n",
        "\n",
        "    Args:\n",
        "        year: Year to download\n",
        "        quarter: Quarter to download (1-4)\n",
        "        force: If True, re-download even if file exists in GCS\n",
        "\n",
        "    Returns:\n",
        "        GCS URI if successful, None otherwise\n",
        "    \"\"\"\n",
        "    gcs_path = get_gcs_path(year, quarter)\n",
        "    gcs_uri = f\"gs://{CONFIG['GCS_BUCKET']}/{gcs_path}\"\n",
        "\n",
        "    # Check if already exists\n",
        "    if not force and file_exists_in_gcs(year, quarter):\n",
        "        print(f\"  ✓ Already in GCS: {year} Q{quarter}\")\n",
        "        return gcs_uri\n",
        "\n",
        "    url = CONFIG['BASE_URL'].format(year, quarter)\n",
        "\n",
        "    # Retry logic for downloads\n",
        "    for attempt in range(CONFIG['MAX_RETRIES']):\n",
        "        try:\n",
        "            print(f\"  ↓ Downloading {year} Q{quarter}... (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
        "            response = requests.get(url, timeout=300)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Extract CSV from ZIP\n",
        "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "                csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
        "                if not csv_files:\n",
        "                    print(f\"  ✗ No CSV found in ZIP for {year} Q{quarter}\")\n",
        "                    return None\n",
        "\n",
        "                csv_data = z.read(csv_files[0])\n",
        "\n",
        "            # Load into pandas for validation\n",
        "            df = pd.read_csv(io.BytesIO(csv_data))\n",
        "\n",
        "            # Validate data quality\n",
        "            issues = validate_dataframe(df, year, quarter)\n",
        "            if issues:\n",
        "                print(f\"  ⚠ Data quality issues for {year} Q{quarter}:\")\n",
        "                for issue in issues:\n",
        "                    print(f\"    - {issue}\")\n",
        "\n",
        "            # Upload to GCS\n",
        "            bucket = storage_client.bucket(CONFIG['GCS_BUCKET'])\n",
        "            blob = bucket.blob(gcs_path)\n",
        "            blob.upload_from_string(csv_data, content_type='text/csv')\n",
        "\n",
        "            print(f\"  ✓ Uploaded to GCS: {year} Q{quarter} ({len(df):,} rows, {len(csv_data) / 1024 / 1024:.1f} MB)\")\n",
        "\n",
        "            # Clean up\n",
        "            del df, csv_data, response\n",
        "            gc.collect()\n",
        "\n",
        "            return gcs_uri\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ✗ Download failed for {year} Q{quarter}: {e}\")\n",
        "            if attempt < CONFIG['MAX_RETRIES'] - 1:\n",
        "                print(f\"    Retrying in {CONFIG['RETRY_DELAY']} seconds...\")\n",
        "                time.sleep(CONFIG['RETRY_DELAY'])\n",
        "            else:\n",
        "                print(f\"    Max retries exceeded. Skipping {year} Q{quarter}.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error processing {year} Q{quarter}: {e}\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_all_files(skip_existing: bool = True) -> List[Tuple[int, int, str]]:\n",
        "    \"\"\"\n",
        "    Download all configured files and upload to GCS.\n",
        "\n",
        "    Returns:\n",
        "        List of (year, quarter, gcs_uri) tuples for successful downloads\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: DOWNLOAD AND STAGE TO GCS\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Get already loaded quarters\n",
        "    loaded_quarters = get_loaded_quarters() if skip_existing else set()\n",
        "    if loaded_quarters:\n",
        "        print(f\"Found {len(loaded_quarters)} quarters already in BigQuery. Will skip these.\\n\")\n",
        "\n",
        "    successful_files = []\n",
        "\n",
        "    total_files = len(CONFIG['YEARS']) * len(CONFIG['QUARTERS'])\n",
        "    with tqdm(total=total_files, desc=\"Overall Progress\") as pbar:\n",
        "        for year in CONFIG['YEARS']:\n",
        "            for quarter in CONFIG['QUARTERS']:\n",
        "                # Skip if already loaded in BigQuery\n",
        "                if (year, quarter) in loaded_quarters:\n",
        "                    print(f\"  ⊘ Skipping {year} Q{quarter} (already in BigQuery)\")\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                gcs_uri = download_and_upload_to_gcs(year, quarter)\n",
        "                if gcs_uri:\n",
        "                    successful_files.append((year, quarter, gcs_uri))\n",
        "\n",
        "                pbar.update(1)\n",
        "                time.sleep(1)  # Be nice to BTS servers\n",
        "\n",
        "    print(f\"\\n✓ Successfully staged {len(successful_files)} files in GCS\")\n",
        "    return successful_files\n",
        "\n",
        "print(\"✓ Download functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_NNL23ca3rT"
      },
      "source": [
        "## Schema Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZmK-8-wa3rU"
      },
      "outputs": [],
      "source": [
        "def analyze_schema(files: List[Tuple[int, int, str]]) -> List[bigquery.SchemaField]:\n",
        "    \"\"\"\n",
        "    Analyze schema across all files and create a unified BigQuery schema.\n",
        "    Tracks schema evolution across years.\n",
        "    IMPORTANT: Preserves the actual column order from CSV files.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: ANALYZE SCHEMA\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    all_columns = set()\n",
        "    schema_by_year = {}\n",
        "    column_order = None  # Will store the actual column order from CSV\n",
        "\n",
        "    print(\"Scanning columns across all files...\")\n",
        "    for year, quarter, gcs_uri in tqdm(files, desc=\"Schema Analysis\"):\n",
        "        bucket = storage_client.bucket(CONFIG['GCS_BUCKET'])\n",
        "        blob = bucket.blob(get_gcs_path(year, quarter))\n",
        "\n",
        "        # Read just the header\n",
        "        sample = blob.download_as_bytes(start=0, end=10000)\n",
        "        df_sample = pd.read_csv(io.BytesIO(sample), nrows=0)\n",
        "\n",
        "        # Get columns as a list (preserves order)\n",
        "        columns_list = list(df_sample.columns)\n",
        "        columns = set(columns_list)\n",
        "        all_columns.update(columns)\n",
        "\n",
        "        # Store the column order from the first file we see\n",
        "        if column_order is None:\n",
        "            column_order = columns_list\n",
        "            print(f\"Using column order from {year} Q{quarter}\")\n",
        "\n",
        "        if year not in schema_by_year:\n",
        "            schema_by_year[year] = columns\n",
        "\n",
        "    # Report schema evolution\n",
        "    print(f\"\\nFound {len(all_columns)} unique columns across all files.\\n\")\n",
        "\n",
        "    print(\"Schema Evolution:\")\n",
        "    sorted_years = sorted(schema_by_year.keys())\n",
        "    baseline_schema = schema_by_year[sorted_years[0]]\n",
        "\n",
        "    for year in sorted_years:\n",
        "        year_schema = schema_by_year[year]\n",
        "        new_cols = year_schema - baseline_schema\n",
        "        removed_cols = baseline_schema - year_schema\n",
        "\n",
        "        if new_cols or removed_cols:\n",
        "            print(f\"  {year}:\")\n",
        "            if new_cols:\n",
        "                print(f\"    + New columns: {', '.join(sorted(new_cols))}\")\n",
        "            if removed_cols:\n",
        "                print(f\"    - Removed columns: {', '.join(sorted(removed_cols))}\")\n",
        "        else:\n",
        "            print(f\"  {year}: No changes from baseline\")\n",
        "\n",
        "    # Create BigQuery schema using the ACTUAL column order from CSV\n",
        "    bq_schema = []\n",
        "\n",
        "    # Add any new columns that weren't in the baseline at the end\n",
        "    all_columns_ordered = column_order + [c for c in all_columns if c not in column_order]\n",
        "\n",
        "    for col in all_columns_ordered:\n",
        "        # Determine BigQuery type\n",
        "        if col in ['ItinID']:\n",
        "            bq_type = 'INT64'\n",
        "        elif col in ['Year', 'OriginAirportID', 'DestAirportID', 'MktID',\n",
        "                     'OriginAirportSeqID', 'DestAirportSeqID', 'OriginCityMarketID', 'DestCityMarketID']:\n",
        "            bq_type = 'INTEGER'\n",
        "        elif col in ['MktFare', 'BulkFare', 'Passengers', 'MktDistance', 'MktMilesFlown', 'NonStopMiles']:\n",
        "            bq_type = 'FLOAT'\n",
        "        elif col in ['Quarter', 'MktCoupons',\n",
        "                     'ItinGeoType', 'MktGeoType', 'MktDistanceGroup', 'OriginWac', 'DestWac']:\n",
        "            bq_type = 'INTEGER'\n",
        "        else:\n",
        "            bq_type = 'STRING'\n",
        "\n",
        "        # Get description\n",
        "        description = COLUMN_DESCRIPTIONS.get(col, \"\")\n",
        "\n",
        "        bq_schema.append(\n",
        "            bigquery.SchemaField(\n",
        "                name=col,\n",
        "                field_type=bq_type,\n",
        "                mode='NULLABLE',\n",
        "                description=description\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(f\"\\n✓ Created BigQuery schema with {len(bq_schema)} columns\")\n",
        "    print(f\"✓ Column order preserved from CSV files\")\n",
        "    return bq_schema\n",
        "\n",
        "print(\"✓ Schema analysis functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kutIiXSra3rU"
      },
      "source": [
        "## BigQuery Load Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aID-ZBRNa3rV"
      },
      "outputs": [],
      "source": [
        "def create_or_update_table(schema: List[bigquery.SchemaField], recreate: bool = False):\n",
        "    \"\"\"\n",
        "    Create or update the BigQuery table with partitioning and clustering.\n",
        "\n",
        "    Args:\n",
        "        schema: BigQuery schema\n",
        "        recreate: If True, drop and recreate the table\n",
        "    \"\"\"\n",
        "    table_id = get_table_id()\n",
        "\n",
        "    # Check if table exists\n",
        "    try:\n",
        "        existing_table = bq_client.get_table(table_id)\n",
        "        if recreate:\n",
        "            print(f\"Dropping existing table: {table_id}\")\n",
        "            bq_client.delete_table(table_id)\n",
        "        else:\n",
        "            print(f\"Table {table_id} already exists. Will append data.\")\n",
        "            return\n",
        "    except Exception:\n",
        "        pass  # Table doesn't exist, will create\n",
        "\n",
        "    # Create table with partitioning and clustering\n",
        "    table = bigquery.Table(table_id, schema=schema)\n",
        "\n",
        "    # Partition by Year (range partitioning)\n",
        "    table.range_partitioning = bigquery.RangePartitioning(\n",
        "        field=\"Year\",\n",
        "        range_=bigquery.PartitionRange(start=2000, end=2030, interval=1)\n",
        "    )\n",
        "\n",
        "    # Cluster by common query columns\n",
        "    table.clustering_fields = [\"Origin\", \"Dest\", \"Quarter\"]\n",
        "\n",
        "    # Set table description\n",
        "    table.description = (\n",
        "        \"DB1B Origin & Destination Survey - Market Data. \"\n",
        "        \"Contains a 10% sample of airline tickets with origin, destination, fare, and routing info. \"\n",
        "        \"Source: US Department of Transportation, Bureau of Transportation Statistics. \"\n",
        "        f\"Loaded on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}.\"\n",
        "    )\n",
        "\n",
        "    table = bq_client.create_table(table)\n",
        "    print(f\"✓ Created table: {table_id}\")\n",
        "    print(f\"  - Partitioned by: Year (range partitioning)\")\n",
        "    print(f\"  - Clustered by: {', '.join(table.clustering_fields)}\")\n",
        "\n",
        "def load_from_gcs(files: List[Tuple[int, int, str]], schema: List[bigquery.SchemaField]):\n",
        "    \"\"\"\n",
        "    Load data from GCS to BigQuery using load jobs.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: LOAD TO BIGQUERY\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    table_id = get_table_id()\n",
        "\n",
        "    # Ensure table exists\n",
        "    try:\n",
        "        bq_client.get_table(table_id)\n",
        "        print(f\"Table {table_id} exists. Appending data...\\n\")\n",
        "    except Exception:\n",
        "        print(f\"Creating table {table_id}...\\n\")\n",
        "        create_or_update_table(schema, recreate=False)\n",
        "\n",
        "    # Load each file\n",
        "    failed_loads = []\n",
        "\n",
        "    for year, quarter, gcs_uri in tqdm(files, desc=\"Loading to BigQuery\"):\n",
        "        try:\n",
        "            print(f\"\\nLoading {year} Q{quarter} from {gcs_uri}\")\n",
        "\n",
        "            job_config = bigquery.LoadJobConfig(\n",
        "                source_format=bigquery.SourceFormat.CSV,\n",
        "                skip_leading_rows=1,\n",
        "                schema=schema,\n",
        "                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "                create_disposition=bigquery.CreateDisposition.CREATE_NEVER,\n",
        "                allow_quoted_newlines=True,\n",
        "                max_bad_records=100  # Allow some malformed rows\n",
        "            )\n",
        "\n",
        "            load_job = bq_client.load_table_from_uri(\n",
        "                gcs_uri,\n",
        "                table_id,\n",
        "                job_config=job_config\n",
        "            )\n",
        "\n",
        "            # Wait for job to complete\n",
        "            load_job.result()\n",
        "\n",
        "            # Get job statistics\n",
        "            stats = load_job._properties['statistics']['load']\n",
        "            output_rows = int(stats['outputRows'])\n",
        "\n",
        "            print(f\"  ✓ Loaded {output_rows:,} rows for {year} Q{quarter}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Failed to load {year} Q{quarter}: {e}\")\n",
        "            failed_loads.append((year, quarter, str(e)))\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LOAD SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"✓ Successfully loaded: {len(files) - len(failed_loads)}/{len(files)} files\")\n",
        "\n",
        "    if failed_loads:\n",
        "        print(f\"\\n✗ Failed loads ({len(failed_loads)}):\")\n",
        "        for year, quarter, error in failed_loads:\n",
        "            print(f\"  - {year} Q{quarter}: {error}\")\n",
        "\n",
        "    # Get final row count\n",
        "    query = f\"SELECT COUNT(*) as total_rows FROM `{table_id}`\"\n",
        "    result = bq_client.query(query).result()\n",
        "    total_rows = list(result)[0].total_rows\n",
        "    print(f\"\\n✓ Total rows in {table_id}: {total_rows:,}\")\n",
        "    print(f\"✓ Estimated total passengers (×10): {total_rows * CONFIG['SAMPLE_RATE']:,}\")\n",
        "\n",
        "print(\"✓ BigQuery load functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D99JwRyJa3rV"
      },
      "source": [
        "## Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBlL6Rk_a3rW"
      },
      "outputs": [],
      "source": [
        "# Execute the pipeline\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"#\" + \" \"*68 + \"#\")\n",
        "print(\"#\" + \" \"*20 + \"DB1B DATA PIPELINE\" + \" \"*20 + \"#\")\n",
        "print(\"#\" + \" \"*68 + \"#\")\n",
        "print(\"#\"*70 + \"\\n\")\n",
        "\n",
        "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Step 1: Download and stage to GCS\n",
        "files = download_all_files(skip_existing=True)\n",
        "\n",
        "if not files:\n",
        "    print(\"\\n⚠ No new files to process. Pipeline complete.\")\n",
        "else:\n",
        "    # Step 2: Analyze schema\n",
        "    schema = analyze_schema(files)\n",
        "\n",
        "    # Step 3: Load to BigQuery\n",
        "    load_from_gcs(files, schema)\n",
        "\n",
        "    print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"#\" + \" \"*68 + \"#\")\n",
        "    print(\"#\" + \" \"*22 + \"PIPELINE COMPLETE\" + \" \"*22 + \"#\")\n",
        "    print(\"#\" + \" \"*68 + \"#\")\n",
        "    print(\"#\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLq9UNkxa3rW"
      },
      "source": [
        "## Example Queries\n",
        "\n",
        "Now that the data is loaded, here are some useful queries to explore it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o-Uv7tna3rW"
      },
      "outputs": [],
      "source": [
        "# Example 1: Top 10 routes by passenger volume (2024)\n",
        "query_top_routes = f\"\"\"\n",
        "SELECT\n",
        "    Origin,\n",
        "    Dest,\n",
        "    SUM(Passengers * {CONFIG['SAMPLE_RATE']}) as estimated_total_passengers,\n",
        "    ROUND(AVG(MktFare), 2) as avg_fare,\n",
        "    COUNT(*) as num_tickets\n",
        "FROM `{get_table_id()}`\n",
        "WHERE Year = 2024\n",
        "GROUP BY Origin, Dest\n",
        "ORDER BY estimated_total_passengers DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Top 10 Routes by Passenger Volume (2024):\")\n",
        "print(query_top_routes)\n",
        "print(\"\\nTo run: bq_client.query(query_top_routes).to_dataframe()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcXXLsxPa3rX"
      },
      "outputs": [],
      "source": [
        "# Example 2: Average fares by carrier\n",
        "query_carrier_fares = f\"\"\"\n",
        "SELECT\n",
        "    TkCarrier as carrier,\n",
        "    COUNT(*) as num_tickets,\n",
        "    ROUND(AVG(MktFare), 2) as avg_fare,\n",
        "    ROUND(STDDEV(MktFare), 2) as fare_stddev,\n",
        "    SUM(Passengers * {CONFIG['SAMPLE_RATE']}) as estimated_total_passengers\n",
        "FROM `{get_table_id()}`\n",
        "WHERE Year = 2024 AND Quarter = 2\n",
        "    AND MktFare > 0\n",
        "GROUP BY TkCarrier\n",
        "HAVING num_tickets > 1000\n",
        "ORDER BY estimated_total_passengers DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"Average Fares by Carrier (2024 Q2):\")\n",
        "print(query_carrier_fares)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Z2BEmZa3rX"
      },
      "outputs": [],
      "source": [
        "# Example 3: Quarterly trends from a specific airport\n",
        "query_airport_trends = f\"\"\"\n",
        "SELECT\n",
        "    Year,\n",
        "    Quarter,\n",
        "    COUNT(DISTINCT Dest) as num_destinations,\n",
        "    SUM(Passengers * {CONFIG['SAMPLE_RATE']}) as estimated_total_passengers,\n",
        "    ROUND(AVG(MktFare), 2) as avg_fare\n",
        "FROM `{get_table_id()}`\n",
        "WHERE Origin = 'JFK'  -- Change to your airport of interest\n",
        "GROUP BY Year, Quarter\n",
        "ORDER BY Year, Quarter\n",
        "\"\"\"\n",
        "\n",
        "print(\"Quarterly Trends from JFK:\")\n",
        "print(query_airport_trends)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ0puP52a3rX"
      },
      "outputs": [],
      "source": [
        "# Example 4: Market concentration analysis\n",
        "query_market_concentration = f\"\"\"\n",
        "WITH route_carriers AS (\n",
        "    SELECT\n",
        "        Origin,\n",
        "        Dest,\n",
        "        TkCarrier,\n",
        "        SUM(Passengers * {CONFIG['SAMPLE_RATE']}) as passengers\n",
        "    FROM `{get_table_id()}`\n",
        "    WHERE Year = 2024\n",
        "    GROUP BY Origin, Dest, TkCarrier\n",
        "),\n",
        "route_totals AS (\n",
        "    SELECT\n",
        "        Origin,\n",
        "        Dest,\n",
        "        SUM(passengers) as total_passengers,\n",
        "        COUNT(DISTINCT TkCarrier) as num_carriers\n",
        "    FROM route_carriers\n",
        "    GROUP BY Origin, Dest\n",
        ")\n",
        "SELECT\n",
        "    Origin,\n",
        "    Dest,\n",
        "    num_carriers,\n",
        "    total_passengers,\n",
        "    ROUND(total_passengers / num_carriers, 0) as avg_passengers_per_carrier\n",
        "FROM route_totals\n",
        "WHERE total_passengers > 100000  -- Major routes only\n",
        "ORDER BY num_carriers ASC, total_passengers DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "print(\"Market Concentration (Routes with Few Competitors):\")\n",
        "print(query_market_concentration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj9nNCnLa3rY"
      },
      "source": [
        "## Data Dictionary Reference\n",
        "\n",
        "Quick reference for key columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMh1k4LHa3rY"
      },
      "outputs": [],
      "source": [
        "# Display key columns and their meanings\n",
        "key_columns = [\n",
        "    'Origin', 'Dest', 'Passengers', 'MktFare', 'BulkFare',\n",
        "    'TkCarrier', 'OpCarrier', 'Year', 'Quarter',\n",
        "    'MktDistance', 'MktCoupons', 'ItinGeoType'\n",
        "]\n",
        "\n",
        "print(\"Key Column Reference:\")\n",
        "print(\"=\" * 80)\n",
        "for col in key_columns:\n",
        "    if col in COLUMN_DESCRIPTIONS:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  {COLUMN_DESCRIPTIONS[col]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nIMPORTANT: Passenger counts are 10% sample. Multiply by 10 for total estimates.\")\n",
        "print(\"MktFare = Fare for this specific market (one leg of journey)\")\n",
        "print(\"BulkFare = Fare for entire itinerary (all legs combined)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}