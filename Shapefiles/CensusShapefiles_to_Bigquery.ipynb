{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis-org/datasets/blob/main/Shapefiles/CensusShapefiles_to_Bigquery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "597bb974",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "597bb974",
        "outputId": "825a314a-a766-44b5-83c5-defdd06f626d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 2.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U -q pip tensorflow geopandas folium mapclassify google-cloud-storage google-cloud-bigquery[pandas] db-dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58acfa61",
      "metadata": {
        "id": "58acfa61"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import geopandas as gpd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.cloud import storage, bigquery\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d006dec",
      "metadata": {
        "id": "5d006dec"
      },
      "source": [
        "The shapefiles are available at https://www2.census.gov/geo/tiger/TIGER2019/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e5801e07",
      "metadata": {
        "id": "e5801e07"
      },
      "outputs": [],
      "source": [
        "# Path to your service account JSON file\n",
        "service_account_json = 'nyu-datasets-77d8cc8e92d6.json'\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_account_json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "s4lF4rC5L12v"
      },
      "id": "s4lF4rC5L12v",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83e26908",
      "metadata": {
        "id": "83e26908"
      },
      "source": [
        "# US States and US Counties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "10d2b9c1",
      "metadata": {
        "id": "10d2b9c1"
      },
      "outputs": [],
      "source": [
        "state_url = 'https://www2.census.gov/geo/tiger/TIGER2019/STATE/tl_2019_us_state.zip'\n",
        "states_gdf = gpd.GeoDataFrame.from_file(state_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c549f2da",
      "metadata": {
        "id": "c549f2da"
      },
      "outputs": [],
      "source": [
        "counties_url = 'https://www2.census.gov/geo/tiger/TIGER2019/COUNTY/tl_2019_us_county.zip'\n",
        "counties_gdf = gpd.GeoDataFrame.from_file(counties_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "acc521ef",
      "metadata": {
        "id": "acc521ef"
      },
      "outputs": [],
      "source": [
        "states = states_gdf['STATEFP'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5ec056b4",
      "metadata": {
        "id": "5ec056b4"
      },
      "outputs": [],
      "source": [
        "counties = counties_gdf.filter(['STATEFP', 'COUNTYFP']).values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9326c14e",
      "metadata": {
        "id": "9326c14e"
      },
      "source": [
        "# Download state, county, tract, water data to Google Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b74f0746",
      "metadata": {
        "id": "b74f0746"
      },
      "outputs": [],
      "source": [
        "def download_file_and_upload_to_gcs(url, local_path, bucket_name, destination_blob_name):\n",
        "    \"\"\"\n",
        "    Downloads a file from a URL and uploads it to Google Cloud Storage.\n",
        "\n",
        "    Args:\n",
        "    url (str): URL of the file to download.\n",
        "    local_path (str): Local path to save the file.\n",
        "    bucket_name (str): Name of the GCS bucket.\n",
        "    destination_blob_name (str): Destination blob name in the GCS bucket.\n",
        "    \"\"\"\n",
        "    # Download file\n",
        "    response = requests.get(url)\n",
        "    with open(local_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Upload to Google Cloud Storage\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    blob.upload_from_filename(local_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371f1d9d",
      "metadata": {
        "id": "371f1d9d"
      },
      "outputs": [],
      "source": [
        "# Download states\n",
        "download_file_and_upload_to_gcs(state_url, 'maps/tl_2019_us_state.zip', 'census_shapefiles', 'states/tl_2019_us_state.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426dc172",
      "metadata": {
        "id": "426dc172"
      },
      "outputs": [],
      "source": [
        "# Download counties\n",
        "download_file_and_upload_to_gcs(counties_url, 'maps/tl_2019_us_county.zip', 'census_shapefiles', 'counties/tl_2019_us_county.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02bc9ba",
      "metadata": {
        "id": "b02bc9ba"
      },
      "outputs": [],
      "source": [
        "# Download tracts\n",
        "for STATEFP in tqdm(states):\n",
        "    filename = f\"tl_2019_{STATEFP}_tract.zip\"\n",
        "    tract_url = f\"https://www2.census.gov/geo/tiger/TIGER2019/TRACT/{filename}\"\n",
        "    download_file_and_upload_to_gcs(tract_url, f'maps/{filename}', 'census_shapefiles', f'tracts/{filename}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f2aaef",
      "metadata": {
        "id": "78f2aaef"
      },
      "outputs": [],
      "source": [
        "# Download blockgroups\n",
        "for STATEFP in tqdm(states):\n",
        "    filename = f\"tl_2019_{STATEFP}_bg.zip\"\n",
        "    tract_url = f\"https://www2.census.gov/geo/tiger/TIGER2019/BG/{filename}\"\n",
        "    download_file_and_upload_to_gcs(tract_url, f'maps/{filename}', 'census_shapefiles', f'blockgroups/{filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b7a8f4",
      "metadata": {
        "id": "22b7a8f4"
      },
      "outputs": [],
      "source": [
        "# Download blocks\n",
        "for STATEFP in tqdm(states):\n",
        "    filename = f\"tl_2019_{STATEFP}_tabblock10.zip\"\n",
        "    tract_url = f\"https://www2.census.gov/geo/tiger/TIGER2019/TABBLOCK/{filename}\"\n",
        "    download_file_and_upload_to_gcs(tract_url, f'maps/{filename}', 'census_shapefiles', f'blocks/{filename}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c77a58",
      "metadata": {
        "id": "23c77a58"
      },
      "outputs": [],
      "source": [
        "# Download water areas\n",
        "for STATEFP, COUNTYFP in tqdm(counties):\n",
        "    filename = f\"tl_2019_{STATEFP+COUNTYFP}_areawater.zip\"\n",
        "    tract_url = f\"https://www2.census.gov/geo/tiger/TIGER2019/AREAWATER/{filename}\"\n",
        "    download_file_and_upload_to_gcs(tract_url, f'maps/{filename}', 'census_shapefiles', f'areawater/{filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a305ae",
      "metadata": {
        "id": "52a305ae"
      },
      "source": [
        "#  Import shapes to Bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7df4e735",
      "metadata": {
        "id": "7df4e735"
      },
      "outputs": [],
      "source": [
        "def upload_shapefile_to_bigquery(bucket_name, shapefile_blob_name, bigquery_dataset_id, bigquery_table_id, temp_dir='temp', STATEFP=None, COUNTYFP=None):\n",
        "    # Initialize Google Cloud clients\n",
        "    storage_client = storage.Client()\n",
        "    bigquery_client = bigquery.Client()\n",
        "\n",
        "    # Create the temporary directory if it does not exist\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Download the shapefile ZIP from the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(shapefile_blob_name)\n",
        "\n",
        "    # Create the full path for the zip file, ensuring all subdirectories exist\n",
        "    zip_path = os.path.join(temp_dir, shapefile_blob_name)\n",
        "    os.makedirs(os.path.dirname(zip_path), exist_ok=True)\n",
        "\n",
        "    blob.download_to_filename(zip_path)\n",
        "\n",
        "    # Extract the ZIP file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "\n",
        "    # Find the .shp file inside the extracted folder\n",
        "    shp_path = None\n",
        "    for root, dirs, files in os.walk(temp_dir):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.shp'):\n",
        "                shp_path = os.path.join(root, filename)\n",
        "                break\n",
        "        if shp_path:\n",
        "            break\n",
        "\n",
        "    if not shp_path:\n",
        "        raise FileNotFoundError(\"No .shp file found in the extracted ZIP.\")\n",
        "\n",
        "    # Read the shapefile into a GeoDataFrame\n",
        "    gdf = gpd.read_file(shp_path)\n",
        "\n",
        "    if STATEFP: gdf['STATEFP'] = STATEFP\n",
        "    if COUNTYFP: gdf['COUNTYFP'] = COUNTYFP\n",
        "\n",
        "    # Convert the geometries to a BigQuery-friendly format (e.g., WKT)\n",
        "    gdf['geometry'] = gdf['geometry'].apply(lambda x: x.wkt)\n",
        "\n",
        "    # Convert the GeoDataFrame to a Pandas DataFrame\n",
        "    df = pd.DataFrame(gdf)\n",
        "\n",
        "    # Create a BigQuery table and upload the DataFrame\n",
        "    table_id = f\"{bigquery_dataset_id}.{bigquery_table_id}\"\n",
        "    job = bigquery_client.load_table_from_dataframe(df, table_id)\n",
        "    job.result()  # Wait for the job to complete\n",
        "\n",
        "    # Clean up: remove temporary files and directories\n",
        "    for root, dirs, files in os.walk(temp_dir, topdown=False):\n",
        "        for name in files:\n",
        "            os.remove(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            os.rmdir(os.path.join(root, name))\n",
        "\n",
        "\n",
        "def convert_wkt_to_geograpphy(tablename):\n",
        "\n",
        "    bigquery_client = bigquery.Client()\n",
        "    # Convert the WKT column to a GEOGRAPHY column and drop the original WKT column\n",
        "    sql = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE `{tablename} AS\n",
        "        SELECT *, ST_GeogFromText(geometry) AS geography\n",
        "        FROM `{tablename}`;\n",
        "        ALTER TABLE `{tablename}\n",
        "        DROP COLUMN geometry;\n",
        "    \"\"\"\n",
        "    query_job = bigquery_client.query(sql)\n",
        "    query_job.result()  # Wait for the query to complete"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_geodataframe_from_bigquery(bucket_name, shapefile_blob_name, temp_dir='temp', STATEFP=None, COUNTYFP=None):\n",
        "    # Initialize Google Cloud clients\n",
        "    storage_client = storage.Client()\n",
        "    bigquery_client = bigquery.Client()\n",
        "\n",
        "    # Create the temporary directory if it does not exist\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Download the shapefile ZIP from the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(shapefile_blob_name)\n",
        "\n",
        "    # Create the full path for the zip file, ensuring all subdirectories exist\n",
        "    zip_path = os.path.join(temp_dir, shapefile_blob_name)\n",
        "    os.makedirs(os.path.dirname(zip_path), exist_ok=True)\n",
        "\n",
        "    blob.download_to_filename(zip_path)\n",
        "\n",
        "    # Extract the ZIP file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "\n",
        "    # Find the .shp file inside the extracted folder\n",
        "    shp_path = None\n",
        "    for root, dirs, files in os.walk(temp_dir):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.shp'):\n",
        "                shp_path = os.path.join(root, filename)\n",
        "                break\n",
        "        if shp_path:\n",
        "            break\n",
        "\n",
        "    if not shp_path:\n",
        "        raise FileNotFoundError(\"No .shp file found in the extracted ZIP.\")\n",
        "\n",
        "    # Read the shapefile into a GeoDataFrame\n",
        "    gdf = gpd.read_file(shp_path)\n",
        "\n",
        "    if STATEFP: gdf['STATEFP'] = STATEFP\n",
        "    if COUNTYFP: gdf['COUNTYFP'] = COUNTYFP\n",
        "\n",
        "    return gdf"
      ],
      "metadata": {
        "id": "A7gkpYvSvPyC"
      },
      "id": "A7gkpYvSvPyC",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_geodfs_to_bucket(geodf_list, bucket_name, destination_blob_name, temp_file='temp.geojson'):\n",
        "    \"\"\"\n",
        "    Combines a list of GeoDataFrames and saves them as one GeoDataFrame in a Google Cloud Storage bucket.\n",
        "\n",
        "    Args:\n",
        "    geodf_list (list): List of GeoDataFrames.\n",
        "    bucket_name (str): Name of the Google Cloud Storage bucket.\n",
        "    destination_blob_name (str): The destination file name in the bucket.\n",
        "    temp_file (str, optional): Temporary local file name to store combined GeoDataFrame. Defaults to 'temp.geojson'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine GeoDataFrames\n",
        "    combined_geodf = gpd.GeoDataFrame(pd.concat(geodf_list, ignore_index=True))\n",
        "\n",
        "    # Save combined GeoDataFrame to a file\n",
        "    combined_geodf.to_file(temp_file, driver='GeoJSON')\n",
        "\n",
        "    # Initialize Google Cloud Storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Upload the file to Google Cloud Storage\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(temp_file)\n",
        "\n",
        "    # Optionally, clean up the local temporary file\n",
        "    os.remove(temp_file)\n",
        "\n",
        "    print(f\"Combined GeoDataFrame uploaded to {bucket_name}/{destination_blob_name}\")\n",
        "\n",
        "# Example usage:\n",
        "# save_geodfs_to_bucket([gdf1, gdf2, gdf3], 'your-bucket-name', 'combined_data.geojson')\n"
      ],
      "metadata": {
        "id": "2s-Aw1MDzY6F"
      },
      "id": "2s-Aw1MDzY6F",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abf2959",
      "metadata": {
        "id": "9abf2959"
      },
      "outputs": [],
      "source": [
        "upload_shapefile_to_bigquery('census_shapefiles', 'states/tl_2019_us_state.zip', 'nyu-datasets.shapefiles', 'us_states')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906153ee",
      "metadata": {
        "id": "906153ee"
      },
      "outputs": [],
      "source": [
        "upload_shapefile_to_bigquery('census_shapefiles', 'counties/tl_2019_us_county.zip', 'nyu-datasets.shapefiles', 'us_counties')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402fad79",
      "metadata": {
        "id": "402fad79"
      },
      "outputs": [],
      "source": [
        "for STATEFP in tqdm(states):\n",
        "    filename = f\"tl_2019_{STATEFP}_tract.zip\"\n",
        "    upload_shapefile_to_bigquery('census_shapefiles', f'tracts/{filename}', 'nyu-datasets.shapefiles', 'us_tracts')\n",
        "\n",
        "convert_wkt_to_geograpphy('nyu-datasets.shapefiles.us_tracts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422740fc",
      "metadata": {
        "id": "422740fc"
      },
      "outputs": [],
      "source": [
        "for STATEFP in tqdm(states):\n",
        "    filename = f\"tl_2019_{STATEFP}_bg.zip\"\n",
        "    upload_shapefile_to_bigquery('census_shapefiles', f'blockgroups/{filename}', 'nyu-datasets.shapefiles', 'us_blockgroups')\n",
        "\n",
        "convert_wkt_to_geograpphy('nyu-datasets.shapefiles.us_blockgroups')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a5a585",
      "metadata": {
        "id": "85a5a585"
      },
      "outputs": [],
      "source": [
        "for STATEFP in tqdm(states):\n",
        "    filename = f\"tl_2019_{STATEFP}_tabblock10.zip\"\n",
        "    upload_shapefile_to_bigquery('census_shapefiles', f'blocks/{filename}', 'nyu-datasets.shapefiles', 'us_blocks')\n",
        "\n",
        "convert_wkt_to_geograpphy('nyu-datasets.shapefiles.us_blocks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09fc150",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e09fc150",
        "outputId": "a9b654fe-9b8d-4e13-b442-1cb4f7649929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/56 [01:02<57:20, 62.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_54_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 2/56 [02:17<1:02:46, 69.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_12_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 3/56 [04:03<1:16:15, 86.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_17_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 4/56 [05:34<1:16:25, 88.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_27_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 5/56 [06:02<56:25, 66.38s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_24_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 6/56 [06:06<37:52, 45.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_44_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 7/56 [06:52<37:00, 45.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_16_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 8/56 [07:02<27:18, 34.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_33_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 9/56 [08:49<44:41, 57.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_37_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 10/56 [09:04<33:38, 43.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_50_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 11/56 [09:12<24:46, 33.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_09_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 12/56 [09:15<17:31, 23.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_10_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 13/56 [09:49<19:18, 26.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_35_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 14/56 [10:47<25:24, 36.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_06_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 15/56 [11:10<22:10, 32.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_34_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 16/56 [12:30<31:01, 46.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_55_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 17/56 [13:40<34:53, 53.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_41_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 18/56 [16:39<57:48, 91.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_31_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 19/56 [18:58<1:05:09, 105.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_42_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 20/56 [20:12<57:39, 96.09s/it]   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_53_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 21/56 [22:13<1:00:32, 103.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_22_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 22/56 [27:35<1:35:49, 169.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_13_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 23/56 [29:49<1:27:10, 158.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_01_areawater.geojson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 24/56 [30:47<1:08:29, 128.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined GeoDataFrame uploaded to census_shapefiles/areawater_states/tl_2019_49_areawater.geojson\n"
          ]
        }
      ],
      "source": [
        "# Download water areas\n",
        "for STATEFP in tqdm(states):\n",
        "  state_gdfs = []\n",
        "  for s, COUNTYFP in counties:\n",
        "    if s!=STATEFP: continue\n",
        "    filename = f\"tl_2019_{STATEFP+COUNTYFP}_areawater.zip\"\n",
        "    gdf = get_geodataframe_from_bigquery('census_shapefiles', f'areawater/{filename}', STATEFP=STATEFP, COUNTYFP=COUNTYFP)\n",
        "    state_gdfs.append(gdf)\n",
        "  save_geodfs_to_bucket(state_gdfs, 'census_shapefiles', f\"areawater_states/tl_2019_{STATEFP}_areawater.geojson\", temp_file='temp.geojson')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    upload_shapefile_to_bigquery('census_shapefiles', f'areawater/{filename}', 'nyu-datasets.shapefiles', 'areawater', STATEFP = STATEFP, COUNTYFP = COUNTYFP)"
      ],
      "metadata": {
        "id": "eziVWhsG07oZ"
      },
      "id": "eziVWhsG07oZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1e33b1a7",
      "metadata": {
        "id": "1e33b1a7"
      },
      "source": [
        "# Area Water"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7402bf",
      "metadata": {
        "id": "4a7402bf"
      },
      "outputs": [],
      "source": [
        "def download_and_process_geodata(STATEFP, COUNTYFP, YEAR=2019):\n",
        "    \"\"\"\n",
        "    Downloads and processes geospatial data for a specified state and county.\n",
        "\n",
        "    Args:\n",
        "    STATEFP (str): State FIPS code.\n",
        "    COUNTYFP (str): County FIPS code.\n",
        "    YEAR (int, optional): Year for the dataset. Defaults to 2019.\n",
        "\n",
        "    Returns:\n",
        "    GeoDataFrame: A GeoDataFrame containing the processed geospatial data.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Creating directory for maps\n",
        "    os.makedirs('maps', exist_ok=True)\n",
        "\n",
        "    # Download the boundaries of tracts\n",
        "    filename = f\"tl_{YEAR}_{STATEFP}_tract.zip\"\n",
        "    tract_url = f\"https://www2.census.gov/geo/tiger/TIGER{YEAR}/TRACT/{filename}\"\n",
        "    download_file(tract_url, f'maps/{filename}')\n",
        "\n",
        "    # Extracting the ZIP file\n",
        "    with zipfile.ZipFile(f'maps/{filename}', 'r') as zip_ref:\n",
        "        zip_ref.extractall('maps')\n",
        "\n",
        "    shapefile = f\"maps/{filename[:-4]}.shp\"\n",
        "    geo_df = gpd.read_file(shapefile)\n",
        "    # Keep only the data for the county of interest\n",
        "    geo_df = geo_df.query(f\"COUNTYFP == '{COUNTYFP}'\")\n",
        "\n",
        "    # Download water areas to avoid including them in maps\n",
        "    water_filename = f\"tl_{YEAR}_{STATEFP+COUNTYFP}_areawater.zip\"\n",
        "    water_url = f\"https://www2.census.gov/geo/tiger/TIGER{YEAR}/AREAWATER/{water_filename}\"\n",
        "    download_file(water_url, f'maps/{water_filename}')\n",
        "\n",
        "    # Extracting the water ZIP file\n",
        "    with zipfile.ZipFile(f'maps/{water_filename}', 'r') as zip_ref:\n",
        "        zip_ref.extractall('maps')\n",
        "\n",
        "    water_shapefile = f\"maps/{water_filename[:-4]}.shp\"\n",
        "    water_geodf = gpd.read_file(water_shapefile)\n",
        "\n",
        "    # Remove water areas from the tracts\n",
        "    geo_df = gpd.overlay(geo_df, water_geodf, how=\"difference\")\n",
        "\n",
        "    # Keep only variables of interest\n",
        "    geo_df = geo_df.filter(['STATEFP', 'COUNTYFP', 'TRACTCE', 'GEOID', 'NAMELSAD', 'ALAND', 'geometry'])\n",
        "\n",
        "    return geo_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb96dc7",
      "metadata": {
        "id": "cdb96dc7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage:\n",
        "geo_df = download_and_process_geodata(\"36\", \"061\") # For New York, Manhattan\n",
        "\n",
        "# Plot the tract boundaries\n",
        "geo_df.plot(\n",
        "    figsize=(10, 10),\n",
        "    color=\"white\",\n",
        "    edgecolor=\"black\",\n",
        "    linewidth=0.5,\n",
        "    zorder=0,\n",
        "    facecolor=\"white\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}