{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis-org/datasets/blob/main/Collisions/collisions_refactored.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ1fsiQJNUow"
      },
      "source": [
        "# NYC Motor Vehicle Collisions Data Pipeline\n",
        "\n",
        "This notebook downloads, cleans, geocodes, and uploads NYC collision data to Google BigQuery.\n",
        "\n",
        "## Data Source\n",
        "[NYC Open Data - Motor Vehicle Collisions - Crashes](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)\n",
        "\n",
        "## Output\n",
        "Two BigQuery tables in `nyu-datasets.collisions`:\n",
        "- **`collisions`**: Main collision records with location, time, and casualty information (~1.9M rows)\n",
        "- **`causes_types`**: Vehicle contributing factors and types, linked by UNIQUE_KEY (~3.6M rows)\n",
        "\n",
        "## Pipeline Steps\n",
        "1. Download raw data from NYC Open Data API\n",
        "2. Clean and standardize column names\n",
        "3. Convert data types (dates, numerics)\n",
        "4. Normalize vehicle types and contributing factors\n",
        "5. Geocode locations using NYC shapefiles (ZIP codes, boroughs, neighborhoods)\n",
        "6. Detect and filter data quality issues\n",
        "7. Upload to BigQuery with proper schema definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-BoDQCZNUox"
      },
      "source": [
        "# =============================================================================\n",
        "# Setup and Authentication\n",
        "# =============================================================================\n",
        "# Install required Google Cloud packages\n",
        "!pip install -q google-cloud-secret-manager google-cloud-bigquery\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Start timing the pipeline\n",
        "from datetime import datetime\n",
        "t_start = datetime.now()\n",
        "print(f\"Pipeline started at: {t_start}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3kbSPSuNUoz"
      },
      "source": [
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "# Render plots inline with retina display\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# BigQuery settings\n",
        "PROJECT_ID = \"nyu-datasets\"\n",
        "DATASET_ID = \"collisions\"\n",
        "\n",
        "# Data source\n",
        "DATA_URL = \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD\"\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LLkcRmTNUo0"
      },
      "source": [
        "## 1. Download Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jYKzd7FNUo1"
      },
      "source": [
        "# Download collision data from NYC Open Data\n",
        "!curl 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD' -o accidents.csv"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwmXPnHJNUo1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data - use object dtype initially to avoid type inference issues\n",
        "df = pd.read_csv(\"accidents.csv\", low_memory=False, dtype='object')\n",
        "\n",
        "# Store original count for later statistics\n",
        "ORIGINAL_RECORD_COUNT = len(df)\n",
        "\n",
        "print(f\"Loaded {ORIGINAL_RECORD_COUNT:,} collision records\")\n",
        "print(f\"Columns: {len(df.columns)}\")\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM4yb2CvNUo2"
      },
      "source": [
        "### Initial Data Exploration\n",
        "\n",
        "Before cleaning, let's examine the raw data structure and identify potential issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXQxP7b9NUo2"
      },
      "source": [
        "# Examine data types - all loaded as object initially\n",
        "print(\"Column data types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGc1Ow6uNUo3"
      },
      "source": [
        "# Check for missing values in key columns\n",
        "key_columns = ['COLLISION_ID', 'CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE']\n",
        "print(\"Missing values in key columns:\")\n",
        "for col in key_columns:\n",
        "    if col in df.columns:\n",
        "        missing = df[col].isna().sum()\n",
        "        pct = 100 * missing / len(df)\n",
        "        print(f\"  {col}: {missing:,} ({pct:.1f}%)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6-WEvNiNUo3"
      },
      "source": [
        "# Borough distribution\n",
        "print(\"Borough distribution:\")\n",
        "df['BOROUGH'].value_counts(dropna=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssieupRJNUo3"
      },
      "source": [
        "# Date range check\n",
        "print(\"Date range in raw data:\")\n",
        "print(f\"  First date: {df['CRASH DATE'].min()}\")\n",
        "print(f\"  Last date: {df['CRASH DATE'].max()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPxh7j83NUo4"
      },
      "source": [
        "## 2. Data Cleaning and Type Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhFdWdJDNUo4"
      },
      "source": [
        "# Rename columns for consistency and clarity\n",
        "column_mapping = {\n",
        "    \"COLLISION_ID\": \"UNIQUE_KEY\",\n",
        "    \"ZIP CODE\": \"ZIPCODE\",\n",
        "    \"NUMBER OF PERSONS INJURED\": \"PERSONS_INJURED\",\n",
        "    \"NUMBER OF PERSONS KILLED\": \"PERSONS_KILLED\",\n",
        "    \"NUMBER OF PEDESTRIANS INJURED\": \"PEDESTRIANS_INJURED\",\n",
        "    \"NUMBER OF PEDESTRIANS KILLED\": \"PEDESTRIANS_KILLED\",\n",
        "    \"NUMBER OF MOTORIST INJURED\": \"MOTORISTS_INJURED\",\n",
        "    \"NUMBER OF MOTORIST KILLED\": \"MOTORISTS_KILLED\",\n",
        "    \"NUMBER OF CYCLIST INJURED\": \"CYCLISTS_INJURED\",\n",
        "    \"NUMBER OF CYCLIST KILLED\": \"CYCLISTS_KILLED\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 1\": \"CAUSE_VEHICLE_1\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 2\": \"CAUSE_VEHICLE_2\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 3\": \"CAUSE_VEHICLE_3\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 4\": \"CAUSE_VEHICLE_4\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 5\": \"CAUSE_VEHICLE_5\",\n",
        "    \"VEHICLE TYPE CODE 1\": \"TYPE_VEHICLE_1\",\n",
        "    \"VEHICLE TYPE CODE 2\": \"TYPE_VEHICLE_2\",\n",
        "    \"VEHICLE TYPE CODE 3\": \"TYPE_VEHICLE_3\",\n",
        "    \"VEHICLE TYPE CODE 4\": \"TYPE_VEHICLE_4\",\n",
        "    \"VEHICLE TYPE CODE 5\": \"TYPE_VEHICLE_5\",\n",
        "}\n",
        "df = df.rename(columns=column_mapping)\n",
        "\n",
        "# Replace spaces with underscores in remaining column names\n",
        "df.columns = df.columns.str.replace(' ', '_')\n",
        "print(\"Columns after renaming:\")\n",
        "print(df.columns.tolist())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQcXxR3XNUo4"
      },
      "source": [
        "### DateTime Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU887eHANUo4"
      },
      "source": [
        "# Combine date and time into a single datetime column\n",
        "df['DATE_TIME'] = pd.to_datetime(\n",
        "    df['CRASH_DATE'] + ' ' + df['CRASH_TIME'],\n",
        "    format=\"%m/%d/%Y %H:%M\"\n",
        ")\n",
        "print(f\"Date range: {df['DATE_TIME'].min()} to {df['DATE_TIME'].max()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMS7557NUo5"
      },
      "source": [
        "### Numeric Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNWRrRv4NUo5"
      },
      "source": [
        "# Convert casualty columns to numeric (consolidated from 8 separate cells)\n",
        "numeric_columns = [\n",
        "    'PERSONS_INJURED', 'PERSONS_KILLED',\n",
        "    'PEDESTRIANS_INJURED', 'PEDESTRIANS_KILLED',\n",
        "    'CYCLISTS_INJURED', 'CYCLISTS_KILLED',\n",
        "    'MOTORISTS_INJURED', 'MOTORISTS_KILLED'\n",
        "]\n",
        "\n",
        "for col in numeric_columns:\n",
        "    df[col] = pd.to_numeric(df[col].fillna(0), downcast='unsigned')\n",
        "\n",
        "print(\"Numeric column stats:\")\n",
        "df[numeric_columns].describe()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWv1up9YNUo5"
      },
      "source": [
        "### Latitude and Longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldJszbtnNUo5"
      },
      "source": [
        "# Handle missing coordinates\n",
        "df['LATITUDE'] = pd.to_numeric(df['LATITUDE'], errors='coerce').fillna(0.0)\n",
        "df['LONGITUDE'] = pd.to_numeric(df['LONGITUDE'], errors='coerce').fillna(0.0)\n",
        "\n",
        "# Create LOCATION string for reference\n",
        "df['LOCATION'] = '(' + df['LATITUDE'].astype(str) + ', ' + df['LONGITUDE'].astype(str) + ')'\n",
        "\n",
        "print(f\"Records with valid coordinates: {(df['LATITUDE'] != 0).sum():,}\")\n",
        "print(f\"Records missing coordinates: {(df['LATITUDE'] == 0).sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRXCqogkNUo6"
      },
      "source": [
        "## 3. Normalize Vehicle Causes and Types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZD4foknNUo6"
      },
      "source": [
        "# Normalize cause and type columns to uppercase\n",
        "cause_type_cols = [\n",
        "    'CAUSE_VEHICLE_1', 'CAUSE_VEHICLE_2', 'CAUSE_VEHICLE_3', 'CAUSE_VEHICLE_4', 'CAUSE_VEHICLE_5',\n",
        "    'TYPE_VEHICLE_1', 'TYPE_VEHICLE_2', 'TYPE_VEHICLE_3', 'TYPE_VEHICLE_4', 'TYPE_VEHICLE_5'\n",
        "]\n",
        "\n",
        "for col in cause_type_cols:\n",
        "    df[col] = df[col].str.upper()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVGlxBfSNUo6"
      },
      "source": [
        "### Create Normalized Causes/Types Table\n",
        "\n",
        "Unpivot the 5 vehicle columns into a single table with one row per vehicle involved in each collision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7OG8cLGNUo6"
      },
      "source": [
        "# Create a normalized table for causes and vehicle types\n",
        "# This unpivots the 5 vehicle columns into rows\n",
        "\n",
        "vehicle_dfs = []\n",
        "for i in range(1, 6):\n",
        "    temp_df = df[['UNIQUE_KEY', f'CAUSE_VEHICLE_{i}', f'TYPE_VEHICLE_{i}']].copy()\n",
        "    temp_df['VEHICLE'] = i\n",
        "    temp_df.columns = ['UNIQUE_KEY', 'CAUSE', 'VEHICLE_TYPE', 'VEHICLE']\n",
        "    vehicle_dfs.append(temp_df)\n",
        "\n",
        "ct_df = pd.concat(vehicle_dfs, ignore_index=True)\n",
        "ct_df = ct_df.dropna(subset=['CAUSE', 'VEHICLE_TYPE'], how='all')\n",
        "ct_df = ct_df.sort_values(['UNIQUE_KEY', 'VEHICLE'])\n",
        "\n",
        "print(f\"Created causes_types table: {len(ct_df):,} rows\")\n",
        "ct_df.head(10)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdjjl8asNUo6"
      },
      "source": [
        "### Standardize Vehicle Types\n",
        "\n",
        "The raw data contains many variations and typos for vehicle types. We normalize these to canonical values using a mapping dictionary.\n",
        "\n",
        "Reference: [NY DMV Vehicle Body Type Codes](https://data.ny.gov/api/assets/83055271-29A6-4ED4-9374-E159F30DB5AE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvkz9MMNUo7"
      },
      "source": [
        "# Vehicle type normalization mapping\n",
        "# Maps variant spellings/abbreviations to canonical types\n",
        "VEHICLE_TYPE_MAPPING = {\n",
        "    'AMBULANCE': ['AM', 'AMB', 'AMBU', 'AMBUL', 'ANBUL', 'AMABU', 'AMULA', 'ABULA', 'AMBULANCE`', 'ALMBULANCE', 'AMUBULANCE', 'AMULANCE'],\n",
        "    'BICYCLE': ['BICYC', 'BIKE'],\n",
        "    'BOX TRUCK': ['FB', 'BOX T', 'BOX', 'BOXTR'],\n",
        "    'BUS': ['BU', 'BS', 'SCHOOL BUS'],\n",
        "    'COMMERCIAL': ['COMME', 'COM', 'COMM', 'COM T', 'COM.', 'COMMM', 'COMER', 'COMIX', 'COMPA', 'COMB', 'COMMU', 'COMM.', 'C0MME'],\n",
        "    'CONVERTIBLE': ['CONV', 'CONVE'],\n",
        "    'DELIVERY TRUCK': ['DELV', 'DEL'],\n",
        "    'DUMP TRUCK': ['DUMP', 'DUMPS', 'DUMPT', 'PUMP', 'DP'],\n",
        "    'E-BIKE': ['E BIK', 'E-BIK', 'E/BIK', 'EBIKE'],\n",
        "    'E-SCOOTER': ['GARBAGE TR'],\n",
        "    'FIRE TRUCK': ['FDNY', 'FIRE', 'FIRET', 'FD NY', 'FD TR'],\n",
        "    'FIRETRUCK': ['FIRE TRUCK', 'FDNY TRUCK'],\n",
        "    'FORK LIFT': ['FORKL', 'FORK'],\n",
        "    'GARBAGE OR REFUSE': ['GARBA'],\n",
        "    'LARGE COM VEH(6 OR MORE TIRES)': ['MULTI-WHEELED VEHICLE'],\n",
        "    'LIVERY VEHICLE': ['LIVER', 'LIMOU', 'LIMO', 'LIMO/', 'OML/'],\n",
        "    'MOTORCYCLE': ['MOTOR', 'MINICYCLE', 'MOTORBIKE'],\n",
        "    'PASSENGER VEHICLE': ['PASS', 'PAS', 'PASSE'],\n",
        "    'PEDICAB': ['PEDIC'],\n",
        "    'PICK-UP TRUCK': ['TKP', 'PK', 'PICK', 'PICK-', 'PICKU', 'PICKUP WITH MOUNTED CAMPER', 'FLAT BED', 'FLAT RACK', 'FLATB', 'FLAT', 'BACK', 'PICK UP TR'],\n",
        "    'POWER SHOVEL': ['P/SH'],\n",
        "    'R/V': ['MOTOR HOME', 'MOTORIZED HOME', 'RV', 'R/V C', 'RV/TR', 'H/WH'],\n",
        "    'ROAD SWEEPER': ['RD/S'],\n",
        "    'SANITATION': ['DSNY', 'DS', 'SANIT'],\n",
        "    'SCOOTER': ['SC', 'SCOO', 'SCOOT', 'MOTER', 'MOPD', 'MOPAD', 'MOPET', 'MOPEN', 'MINIBIKE', 'MOPED'],\n",
        "    'SEDAN': ['2 DR SEDAN', '4 DR SEDAN', 'SUDAN', 'SE', '4DS', '4DSD', '3-DOOR'],\n",
        "    'SMALL COM VEH(4 TIRES)': ['SMALL', 'SMALL COM VEH(4 TIRES) '],\n",
        "    'SNOW PLOW': ['SP'],\n",
        "    'SPORT UTILITY / STATION WAGON': ['STATION WAGON/SPORT UTILITY VEHICLE', 'SUBN', 'SUBN/', 'WAGON'],\n",
        "    'TANK TRUCK': ['TANK', 'TANKE', 'TANKER'],\n",
        "    'TOW TRUCK / WRECKER': ['TOW T', 'TOW', 'TOW TRUCK', 'TOWTR', 'TOW-T', 'TOWIN', 'TOWER', 'G TOW'],\n",
        "    'TRACTOR TRUCK': ['TRACTOR TRUCK DIESEL', 'TRACTOR TRUCK GASOLINE', 'TRAC.', 'TRAC', 'TRACTOR TR', 'TRACTOR'],\n",
        "    'TRAILER': ['TRAIL', 'TR', 'TRL', 'TRLR'],\n",
        "    'TRUCK': ['SEMI', 'SEMI-', 'MACK', 'TK', 'TRACT', 'TRK', 'TRACK'],\n",
        "    'UNKNOWN': ['OTHER', 'UNK', 'UNK,', 'UNKN', 'UNKNO', 'UNKNOWN', 'UNKOW', 'UNNKO'],\n",
        "    'USPS': ['US PO', 'USPOS', 'USPS2', 'USPST', 'U.S P', 'U.S.', 'USPS TRUCK'],\n",
        "    'VAN': ['VAN CAMPER', 'VAN T', 'VAN F', 'VAN A', 'VAN W', 'VAN/B', 'VAB', 'VANG', 'VAN C', 'VAN/T', 'VANETTE', 'VAN`', 'VAV', 'VN', 'VAN (', 'ENCLOSED BODY - REMOVABLE ENCLOSURE', 'ENCLOSED BODY - NONREMOVABLE ENCLOSURE', 'ENCLO'],\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB4zWHF8NUo7"
      },
      "source": [
        "# Apply vehicle type normalization\n",
        "for canonical, variants in VEHICLE_TYPE_MAPPING.items():\n",
        "    ct_df['VEHICLE_TYPE'] = ct_df['VEHICLE_TYPE'].replace(variants, canonical)\n",
        "\n",
        "print(f\"Unique vehicle types after normalization: {ct_df['VEHICLE_TYPE'].nunique()}\")\n",
        "print(\"\\nTop 15 vehicle types:\")\n",
        "ct_df['VEHICLE_TYPE'].value_counts().head(15)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whBPW9hRNUo7"
      },
      "source": [
        "# Drop the individual vehicle columns from main dataframe (now in ct_df)\n",
        "vehicle_cols_to_drop = [\n",
        "    'CAUSE_VEHICLE_1', 'TYPE_VEHICLE_1',\n",
        "    'CAUSE_VEHICLE_2', 'TYPE_VEHICLE_2',\n",
        "    'CAUSE_VEHICLE_3', 'TYPE_VEHICLE_3',\n",
        "    'CAUSE_VEHICLE_4', 'TYPE_VEHICLE_4',\n",
        "    'CAUSE_VEHICLE_5', 'TYPE_VEHICLE_5'\n",
        "]\n",
        "df = df.drop(columns=vehicle_cols_to_drop)\n",
        "print(f\"Main dataframe columns: {len(df.columns)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hfqZ5k-NUo8"
      },
      "source": [
        "## 4. Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTJQUzTKNUo8"
      },
      "source": [
        "# Identify records with data quality issues\n",
        "\n",
        "# 1. Incorrect sum of injured people\n",
        "check_injured = (\n",
        "    df.PEDESTRIANS_INJURED + df.CYCLISTS_INJURED + df.MOTORISTS_INJURED != df.PERSONS_INJURED\n",
        ")\n",
        "incorrect_injured = set(df[check_injured].UNIQUE_KEY.values)\n",
        "print(f\"Records with incorrect injured totals: {len(incorrect_injured):,}\")\n",
        "\n",
        "# 2. Incorrect sum of killed people\n",
        "check_killed = (\n",
        "    df.PEDESTRIANS_KILLED + df.CYCLISTS_KILLED + df.MOTORISTS_KILLED != df.PERSONS_KILLED\n",
        ")\n",
        "incorrect_killed = set(df[check_killed].UNIQUE_KEY.values)\n",
        "print(f\"Records with incorrect killed totals: {len(incorrect_killed):,}\")\n",
        "\n",
        "# 3. No vehicle/cause entries\n",
        "nocause = set(df.UNIQUE_KEY.values) - set(ct_df.UNIQUE_KEY.values)\n",
        "print(f\"Records with no vehicle information: {len(nocause):,}\")\n",
        "\n",
        "# 4. Inconsistent vehicle numbering\n",
        "vehicle_counts = ct_df.groupby('UNIQUE_KEY').agg(\n",
        "    count=('VEHICLE', 'count'),\n",
        "    max_num=('VEHICLE', 'max')\n",
        ")\n",
        "incorrect_vehicles = set(vehicle_counts[vehicle_counts['count'] != vehicle_counts['max_num']].index)\n",
        "print(f\"Records with vehicle numbering gaps: {len(incorrect_vehicles):,}\")\n",
        "\n",
        "# Combine all issues\n",
        "records_to_exclude = incorrect_injured | incorrect_killed | nocause | incorrect_vehicles\n",
        "print(f\"\\nTotal unique records with issues: {len(records_to_exclude):,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcKE2RF7NUo8"
      },
      "source": [
        "### Data Quality Visualization\n",
        "\n",
        "Visualize the temporal distribution of data quality issues to understand if problems are systematic or random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XD8tuX0NUo8"
      },
      "source": [
        "# Plot data quality issues over time\n",
        "# Create a dataframe of problematic records\n",
        "problem_df = df[df['UNIQUE_KEY'].isin(records_to_exclude)].copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 4))\n",
        "problem_df.set_index('DATE_TIME').resample('1W')['UNIQUE_KEY'].count().plot(\n",
        "    ax=ax,\n",
        "    title='Data Quality Issues by Week',\n",
        "    ylabel='Number of problematic records'\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTime period with most issues: {problem_df.set_index('DATE_TIME').resample('1M')['UNIQUE_KEY'].count().idxmax()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOMec2r_NUo9"
      },
      "source": [
        "# Breakdown of issue types\n",
        "print(\"Issue Type Breakdown:\")\n",
        "print(f\"  Incorrect injured totals: {len(incorrect_injured):,}\")\n",
        "print(f\"  Incorrect killed totals:  {len(incorrect_killed):,}\")\n",
        "print(f\"  Missing vehicle info:     {len(nocause):,}\")\n",
        "print(f\"  Vehicle numbering gaps:   {len(incorrect_vehicles):,}\")\n",
        "print(f\"  ─────────────────────────────────\")\n",
        "print(f\"  Total unique (with overlap): {len(records_to_exclude):,}\")\n",
        "\n",
        "# Check overlap between categories\n",
        "from itertools import combinations\n",
        "categories = [\n",
        "    ('incorrect_injured', incorrect_injured),\n",
        "    ('incorrect_killed', incorrect_killed),\n",
        "    ('nocause', nocause),\n",
        "    ('incorrect_vehicles', incorrect_vehicles)\n",
        "]\n",
        "print(\"\\nOverlap between categories:\")\n",
        "for (n1, s1), (n2, s2) in combinations(categories, 2):\n",
        "    overlap = len(s1 & s2)\n",
        "    if overlap > 0:\n",
        "        print(f\"  {n1} ∩ {n2}: {overlap:,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcxUysmiNUo-"
      },
      "source": [
        "## 5. Geocoding with NYC Shapefiles\n",
        "\n",
        "Use spatial joins with NYC shapefiles to:\n",
        "1. Identify the neighborhood and borough for each collision location\n",
        "2. Detect and fill missing ZIP codes\n",
        "3. Filter out collisions outside NYC boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjxQcZyqNUo-"
      },
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Convert collision coordinates to GeoDataFrame\n",
        "geometry = [Point(xy) for xy in zip(df['LONGITUDE'].astype(float), df['LATITUDE'].astype(float))]\n",
        "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
        "print(f\"Created GeoDataFrame with {len(gdf):,} records\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVQlbmlmNUo-"
      },
      "source": [
        "%%time\n",
        "# Load NYC neighborhood shapefile\n",
        "shapefile_url = 'https://data.cityofnewyork.us/resource/9nt8-h7nd.geojson'\n",
        "df_nyc = gpd.GeoDataFrame.from_file(shapefile_url)\n",
        "df_nyc = df_nyc.to_crs(\"EPSG:4326\")\n",
        "\n",
        "print(f\"Loaded {len(df_nyc)} NYC neighborhoods\")\n",
        "df_nyc[['ntaname', 'boro_name']].head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l6YG59GNUo-"
      },
      "source": [
        "%%time\n",
        "# Spatial join: match each collision with its neighborhood\n",
        "# Using left join to preserve all collision records\n",
        "gdf = gpd.sjoin(gdf, df_nyc[['ntaname', 'boro_name', 'geometry']], how='left', predicate='within')\n",
        "\n",
        "# Rename columns for clarity\n",
        "gdf = gdf.rename(columns={'ntaname': 'NEIGHBORHOOD', 'boro_name': 'DETECTED_BOROUGH'})\n",
        "\n",
        "# Drop unnecessary columns from the join\n",
        "if 'index_right' in gdf.columns:\n",
        "    gdf = gdf.drop(columns=['index_right'])\n",
        "\n",
        "print(f\"Records matched to neighborhoods: {gdf['NEIGHBORHOOD'].notna().sum():,}\")\n",
        "print(f\"Records outside NYC boundaries: {gdf['NEIGHBORHOOD'].isna().sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GV9W_nwNUo-"
      },
      "source": [
        "%%time\n",
        "# Load NYC ZIP code shapefile\n",
        "shapefile_zip = 'https://data.cityofnewyork.us/resource/pri4-ifjk.geojson'\n",
        "df_zip = gpd.GeoDataFrame.from_file(shapefile_zip)\n",
        "df_zip = df_zip.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Spatial join to get ZIP codes\n",
        "gdf = gpd.sjoin(gdf, df_zip[['modzcta', 'geometry']], how='left', predicate='within')\n",
        "gdf = gdf.rename(columns={'modzcta': 'DETECTED_ZIPCODE'})\n",
        "\n",
        "if 'index_right' in gdf.columns:\n",
        "    gdf = gdf.drop(columns=['index_right'])\n",
        "\n",
        "print(f\"Records matched to ZIP codes: {gdf['DETECTED_ZIPCODE'].notna().sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgpaiSXfNUo_"
      },
      "source": [
        "### Post-Geocoding Analysis\n",
        "\n",
        "Compare detected locations with originally reported values and identify discrepancies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGGCGoBoNUo_"
      },
      "source": [
        "# Analyze location detection results\n",
        "print(\"=== Location Detection Summary ===\")\n",
        "print(f\"Total records: {len(gdf):,}\")\n",
        "print(f\"\\nNeighborhood detection:\")\n",
        "print(f\"  - Detected: {gdf['NEIGHBORHOOD'].notna().sum():,}\")\n",
        "print(f\"  - Missing: {gdf['NEIGHBORHOOD'].isna().sum():,}\")\n",
        "\n",
        "print(f\"\\nBorough comparison:\")\n",
        "# Records where detected borough differs from reported\n",
        "borough_mismatch = (\n",
        "    gdf['DETECTED_BOROUGH'].notna() &\n",
        "    gdf['BOROUGH'].notna() &\n",
        "    (gdf['DETECTED_BOROUGH'].str.upper() != gdf['BOROUGH'].str.upper())\n",
        ")\n",
        "print(f\"  - Mismatches: {borough_mismatch.sum():,}\")\n",
        "\n",
        "print(f\"\\nZIP code comparison:\")\n",
        "zip_mismatch = (\n",
        "    gdf['DETECTED_ZIPCODE'].notna() &\n",
        "    gdf['ZIPCODE'].notna() &\n",
        "    (gdf['DETECTED_ZIPCODE'] != gdf['ZIPCODE'])\n",
        ")\n",
        "print(f\"  - Mismatches: {zip_mismatch.sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4XW-w5CNUpA"
      },
      "source": [
        "### Geocoding Deep Dive\n",
        "\n",
        "Detailed analysis of geocoding results, identifying patterns in failures and discrepancies.\n",
        "\n",
        "This section helps understand:\n",
        "- Where geocoding fails (outside NYC, water, parks, etc.)\n",
        "- Common locations with missing data\n",
        "- Systematic discrepancies between reported and detected locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duvth5UqNUpA"
      },
      "source": [
        "# Define NYC bounding box for filtering\n",
        "# Approximate bounds: 40.4°N to 40.95°N, -74.3°W to -73.65°W\n",
        "NYC_BOUNDS = {\n",
        "    'lat_min': 40.4,\n",
        "    'lat_max': 40.95,\n",
        "    'lon_min': -74.3,\n",
        "    'lon_max': -73.65\n",
        "}\n",
        "\n",
        "# Check which records fall outside NYC bounds\n",
        "outside_nyc_mask = ~(\n",
        "    (gdf['LATITUDE'] >= NYC_BOUNDS['lat_min']) &\n",
        "    (gdf['LATITUDE'] <= NYC_BOUNDS['lat_max']) &\n",
        "    (gdf['LONGITUDE'] >= NYC_BOUNDS['lon_min']) &\n",
        "    (gdf['LONGITUDE'] <= NYC_BOUNDS['lon_max'])\n",
        ")\n",
        "\n",
        "# Records with no/zero coordinates\n",
        "no_coords_mask = (gdf['LATITUDE'] == 0) | (gdf['LONGITUDE'] == 0)\n",
        "\n",
        "print(\"=== Coordinate Analysis ===\")\n",
        "print(f\"Records with zero coordinates: {no_coords_mask.sum():,}\")\n",
        "print(f\"Records outside NYC bounds: {(outside_nyc_mask & ~no_coords_mask).sum():,}\")\n",
        "print(f\"Records within NYC bounds: {(~outside_nyc_mask & ~no_coords_mask).sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pde_hwiNUpB"
      },
      "source": [
        "# Analyze records outside NYC boundaries (excluding zero coords)\n",
        "outside_records = gdf[outside_nyc_mask & ~no_coords_mask].copy()\n",
        "\n",
        "if len(outside_records) > 0:\n",
        "    print(f\"\\n=== Records Outside NYC Bounds ({len(outside_records):,}) ===\")\n",
        "    print(\"\\nSample coordinates:\")\n",
        "    print(outside_records[['LATITUDE', 'LONGITUDE', 'BOROUGH', 'ON_STREET_NAME']].head(10))\n",
        "\n",
        "    print(\"\\nReported boroughs for outside-NYC records:\")\n",
        "    print(outside_records['BOROUGH'].value_counts(dropna=False).head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWrqclyXNUpB"
      },
      "source": [
        "# Visualize collision density on NYC map\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# Plot NYC neighborhoods as base map\n",
        "df_nyc.plot(\n",
        "    ax=ax,\n",
        "    color='white',\n",
        "    edgecolor='gray',\n",
        "    linewidth=0.5,\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# Plot collisions that were successfully geocoded\n",
        "valid_coords = gdf[\n",
        "    (gdf['NEIGHBORHOOD'].notna()) &\n",
        "    (gdf['LATITUDE'] != 0)\n",
        "].copy()\n",
        "\n",
        "valid_coords.plot(\n",
        "    ax=ax,\n",
        "    kind='scatter',\n",
        "    x='LONGITUDE',\n",
        "    y='LATITUDE',\n",
        "    s=0.1,\n",
        "    alpha=0.02,\n",
        "    c='blue'\n",
        ")\n",
        "\n",
        "ax.set_title(f'NYC Motor Vehicle Collisions\\n({len(valid_coords):,} geocoded records)', fontsize=14)\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGVYo7C8NUpC"
      },
      "source": [
        "#### Records Without Detected Borough\n",
        "\n",
        "These records have valid NYC coordinates but fall outside neighborhood polygons (water, parks, highway medians, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGPCZEX5NUpC"
      },
      "source": [
        "# Records within NYC bounds but no borough detected\n",
        "no_borough = gdf[\n",
        "    ~outside_nyc_mask &\n",
        "    ~no_coords_mask &\n",
        "    gdf['NEIGHBORHOOD'].isna()\n",
        "].copy()\n",
        "\n",
        "print(f\"=== No Borough Detected ({len(no_borough):,} records) ===\")\n",
        "\n",
        "if len(no_borough) > 0:\n",
        "    print(\"\\nMost common streets for unmatched locations:\")\n",
        "    print(no_borough['ON_STREET_NAME'].value_counts().head(15))\n",
        "\n",
        "    print(\"\\nMost common coordinate clusters:\")\n",
        "    # Round to 3 decimal places to find clusters\n",
        "    no_borough['LOC_CLUSTER'] = (\n",
        "        no_borough['LATITUDE'].round(3).astype(str) + ', ' +\n",
        "        no_borough['LONGITUDE'].round(3).astype(str)\n",
        "    )\n",
        "    print(no_borough['LOC_CLUSTER'].value_counts().head(10))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADSNVRgtNUpD"
      },
      "source": [
        "# Map of unmatched locations\n",
        "if len(no_borough) > 0 and len(no_borough) < 50000:\n",
        "    fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "    df_nyc.plot(ax=ax, color='lightgray', edgecolor='gray', linewidth=0.5)\n",
        "\n",
        "    no_borough.plot(\n",
        "        ax=ax,\n",
        "        kind='scatter',\n",
        "        x='LONGITUDE',\n",
        "        y='LATITUDE',\n",
        "        s=1,\n",
        "        alpha=0.3,\n",
        "        c='red',\n",
        "        label='No borough detected'\n",
        "    )\n",
        "\n",
        "    ax.set_title(f'Collisions Without Detected Borough ({len(no_borough):,} records)')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xqkzhQpNUpD"
      },
      "source": [
        "#### Borough Discrepancies\n",
        "\n",
        "Cases where the detected borough differs from the reported borough. These could indicate:\n",
        "- Data entry errors in the original report\n",
        "- Boundary edge cases\n",
        "- Geocoding precision issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r4il4zfNUpE"
      },
      "source": [
        "# Analyze borough discrepancies\n",
        "gdf['BOROUGH_UPPER'] = gdf['BOROUGH'].str.upper().str.strip()\n",
        "gdf['DETECTED_UPPER'] = gdf['DETECTED_BOROUGH'].str.upper().str.strip()\n",
        "\n",
        "borough_mismatch = gdf[\n",
        "    gdf['BOROUGH_UPPER'].notna() &\n",
        "    gdf['DETECTED_UPPER'].notna() &\n",
        "    (gdf['BOROUGH_UPPER'] != gdf['DETECTED_UPPER'])\n",
        "].copy()\n",
        "\n",
        "print(f\"=== Borough Discrepancies ({len(borough_mismatch):,} records) ===\")\n",
        "\n",
        "if len(borough_mismatch) > 0:\n",
        "    # Cross-tabulation\n",
        "    print(\"\\nReported vs Detected Borough:\")\n",
        "    crosstab = pd.crosstab(\n",
        "        borough_mismatch['BOROUGH_UPPER'],\n",
        "        borough_mismatch['DETECTED_UPPER'],\n",
        "        margins=True\n",
        "    )\n",
        "    print(crosstab)\n",
        "\n",
        "    print(\"\\nMost common mismatch locations:\")\n",
        "    borough_mismatch['LOC_CLUSTER'] = (\n",
        "        borough_mismatch['LATITUDE'].round(3).astype(str) + ', ' +\n",
        "        borough_mismatch['LONGITUDE'].round(3).astype(str)\n",
        "    )\n",
        "    print(borough_mismatch['LOC_CLUSTER'].value_counts().head(10))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKSWsUgHNUpE"
      },
      "source": [
        "# Map of borough discrepancies\n",
        "if len(borough_mismatch) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "    df_nyc.plot(ax=ax, color='white', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    borough_mismatch.plot(\n",
        "        ax=ax,\n",
        "        kind='scatter',\n",
        "        x='LONGITUDE',\n",
        "        y='LATITUDE',\n",
        "        s=2,\n",
        "        alpha=0.5,\n",
        "        c='orange',\n",
        "        label='Borough mismatch'\n",
        "    )\n",
        "\n",
        "    ax.set_title(f'Borough Discrepancies ({len(borough_mismatch):,} records)\\nReported ≠ Detected Borough')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6-gnakLNUpF"
      },
      "source": [
        "#### ZIP Code Discrepancies\n",
        "\n",
        "Similar analysis for ZIP code mismatches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aHfKtl7NUpF"
      },
      "source": [
        "# Analyze ZIP code discrepancies\n",
        "zip_mismatch = gdf[\n",
        "    gdf['ZIPCODE'].notna() &\n",
        "    gdf['DETECTED_ZIPCODE'].notna() &\n",
        "    (gdf['ZIPCODE'] != gdf['DETECTED_ZIPCODE'])\n",
        "].copy()\n",
        "\n",
        "print(f\"=== ZIP Code Discrepancies ({len(zip_mismatch):,} records) ===\")\n",
        "\n",
        "if len(zip_mismatch) > 0:\n",
        "    print(\"\\nMost common ZIP mismatches (Reported → Detected):\")\n",
        "    zip_mismatch['ZIP_PAIR'] = zip_mismatch['ZIPCODE'] + ' → ' + zip_mismatch['DETECTED_ZIPCODE']\n",
        "    print(zip_mismatch['ZIP_PAIR'].value_counts().head(15))\n",
        "\n",
        "    print(\"\\nMost common streets with ZIP discrepancies:\")\n",
        "    print(zip_mismatch['ON_STREET_NAME'].value_counts().head(10))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5-C9I20NUpF"
      },
      "source": [
        "# Summary statistics after geocoding analysis\n",
        "print(\"=\" * 60)\n",
        "print(\"GEOCODING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal records: {len(gdf):,}\")\n",
        "print(f\"\\nCoordinate issues:\")\n",
        "print(f\"  Zero coordinates: {no_coords_mask.sum():,}\")\n",
        "print(f\"  Outside NYC bounds: {(outside_nyc_mask & ~no_coords_mask).sum():,}\")\n",
        "print(f\"\\nGeocoding results (for valid coordinates):\")\n",
        "print(f\"  Successfully matched to neighborhood: {gdf['NEIGHBORHOOD'].notna().sum():,}\")\n",
        "print(f\"  No neighborhood match: {(gdf['NEIGHBORHOOD'].isna() & ~no_coords_mask & ~outside_nyc_mask).sum():,}\")\n",
        "print(f\"\\nDiscrepancies:\")\n",
        "print(f\"  Borough mismatches: {len(borough_mismatch):,}\")\n",
        "print(f\"  ZIP code mismatches: {len(zip_mismatch):,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XsqHo0NUpG"
      },
      "source": [
        "## 6. Prepare Final Dataset\n",
        "\n",
        "Filter out problematic records and prepare the final columns for BigQuery upload."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x69wM3HzNUpG"
      },
      "source": [
        "# Use detected values where available, fall back to reported values\n",
        "gdf['ZIPCODE'] = gdf['DETECTED_ZIPCODE'].fillna(gdf['ZIPCODE'])\n",
        "gdf['BOROUGH'] = gdf['DETECTED_BOROUGH'].fillna(gdf['BOROUGH'])\n",
        "\n",
        "# Keep reported values for reference\n",
        "gdf['REPORTED_ZIPCODE'] = df['ZIPCODE']\n",
        "gdf['REPORTED_BOROUGH'] = df['BOROUGH']\n",
        "\n",
        "# Filter to records within NYC boundaries (have a detected neighborhood)\n",
        "gdf_nyc = gdf[gdf['NEIGHBORHOOD'].notna()].copy()\n",
        "print(f\"Records within NYC: {len(gdf_nyc):,}\")\n",
        "\n",
        "# Ensure ZIPCODE is 5 characters with leading zeros\n",
        "gdf_nyc['ZIPCODE'] = gdf_nyc['ZIPCODE'].astype(str).str.zfill(5)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxP4wNK0NUpG"
      },
      "source": [
        "# Remove records with data quality issues (identified earlier)\n",
        "# Note: records_to_exclude was defined in the Data Quality section\n",
        "final_df = gdf_nyc[~gdf_nyc['UNIQUE_KEY'].isin(records_to_exclude)].copy()\n",
        "\n",
        "# Remove duplicate UNIQUE_KEYs (keep first occurrence)\n",
        "duplicate_keys = final_df['UNIQUE_KEY'].value_counts()\n",
        "duplicate_keys = duplicate_keys[duplicate_keys > 1].index\n",
        "final_df = final_df[~final_df['UNIQUE_KEY'].isin(duplicate_keys)]\n",
        "\n",
        "print(f\"Final dataset: {len(final_df):,} collision records\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjbdFB-ENUpH"
      },
      "source": [
        "# Select and order columns for the collisions table\n",
        "collisions_columns = [\n",
        "    'UNIQUE_KEY', 'DATE_TIME',\n",
        "    'LATITUDE', 'LONGITUDE',\n",
        "    'ZIPCODE', 'NEIGHBORHOOD', 'BOROUGH',\n",
        "    'PERSONS_INJURED', 'PERSONS_KILLED',\n",
        "    'PEDESTRIANS_INJURED', 'PEDESTRIANS_KILLED',\n",
        "    'CYCLISTS_INJURED', 'CYCLISTS_KILLED',\n",
        "    'MOTORISTS_INJURED', 'MOTORISTS_KILLED',\n",
        "    'ON_STREET_NAME', 'CROSS_STREET_NAME', 'OFF_STREET_NAME',\n",
        "    'REPORTED_ZIPCODE', 'REPORTED_BOROUGH'\n",
        "]\n",
        "\n",
        "# Ensure all columns exist and handle geometry column\n",
        "final_df = final_df[collisions_columns].copy()\n",
        "\n",
        "# Convert to regular DataFrame (drop geometry)\n",
        "final_df = pd.DataFrame(final_df)\n",
        "\n",
        "print(\"Final collisions table schema:\")\n",
        "print(final_df.dtypes)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g2by618NUpH"
      },
      "source": [
        "# Filter causes_types to only include records in the final dataset\n",
        "vehicle_causes = ct_df[ct_df['UNIQUE_KEY'].isin(final_df['UNIQUE_KEY'])].copy()\n",
        "vehicle_causes = vehicle_causes.reset_index(drop=True)\n",
        "\n",
        "print(f\"Final causes_types table: {len(vehicle_causes):,} rows\")\n",
        "print(f\"Average vehicles per collision: {len(vehicle_causes) / len(final_df):.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC4x-NTDNUpH"
      },
      "source": [
        "### Contributing Factor & Vehicle Type Analysis\n",
        "\n",
        "Examine the distribution of contributing factors and vehicle types before export.\n",
        "This helps validate the normalization and identify any remaining data quality issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SULtslkcNUpH"
      },
      "source": [
        "# Contributing factor distribution\n",
        "print(\"=== Top 20 Contributing Factors ===\")\n",
        "cause_counts = vehicle_causes['CAUSE'].value_counts()\n",
        "print(cause_counts.head(20))\n",
        "\n",
        "print(f\"\\nTotal unique contributing factors: {vehicle_causes['CAUSE'].nunique()}\")\n",
        "print(f\"\\nRecords with 'UNSPECIFIED': {(vehicle_causes['CAUSE'] == 'UNSPECIFIED').sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwD5JnszNUpI"
      },
      "source": [
        "# Vehicle type distribution after normalization\n",
        "print(\"=== Top 20 Vehicle Types (After Normalization) ===\")\n",
        "type_counts = vehicle_causes['VEHICLE_TYPE'].value_counts()\n",
        "print(type_counts.head(20))\n",
        "\n",
        "print(f\"\\nTotal unique vehicle types: {vehicle_causes['VEHICLE_TYPE'].nunique()}\")\n",
        "print(f\"\\nRecords with 'UNKNOWN': {(vehicle_causes['VEHICLE_TYPE'] == 'UNKNOWN').sum():,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da282Wn0NUpI"
      },
      "source": [
        "# Visualize top contributing factors\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Top 10 causes\n",
        "cause_counts.head(10).plot(\n",
        "    kind='barh',\n",
        "    ax=axes[0],\n",
        "    color='steelblue'\n",
        ")\n",
        "axes[0].set_title('Top 10 Contributing Factors')\n",
        "axes[0].set_xlabel('Number of Records')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Top 10 vehicle types\n",
        "type_counts.head(10).plot(\n",
        "    kind='barh',\n",
        "    ax=axes[1],\n",
        "    color='darkgreen'\n",
        ")\n",
        "axes[1].set_title('Top 10 Vehicle Types')\n",
        "axes[1].set_xlabel('Number of Records')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx6SnxgbNUpI"
      },
      "source": [
        "# Cross-tabulation: Top causes by vehicle type\n",
        "print(\"=== Contributing Factors by Vehicle Type (Top 5 each) ===\")\n",
        "top_causes = cause_counts.head(5).index.tolist()\n",
        "top_types = type_counts.head(5).index.tolist()\n",
        "\n",
        "crosstab = pd.crosstab(\n",
        "    vehicle_causes[vehicle_causes['CAUSE'].isin(top_causes)]['CAUSE'],\n",
        "    vehicle_causes[vehicle_causes['VEHICLE_TYPE'].isin(top_types)]['VEHICLE_TYPE']\n",
        ")\n",
        "print(crosstab)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeu0pQpqNUpI"
      },
      "source": [
        "# Vehicles per collision distribution\n",
        "vehicles_per_collision = vehicle_causes.groupby('UNIQUE_KEY').size()\n",
        "\n",
        "print(\"=== Vehicles per Collision ===\")\n",
        "print(vehicles_per_collision.value_counts().sort_index())\n",
        "\n",
        "print(f\"\\nAverage vehicles per collision: {vehicles_per_collision.mean():.2f}\")\n",
        "print(f\"Max vehicles in a single collision: {vehicles_per_collision.max()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrQh2rw0NUpJ"
      },
      "source": [
        "## 7. Export to BigQuery\n",
        "\n",
        "Upload the processed data to Google BigQuery with proper schema definitions including column descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo-ksElyNUpJ"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import SchemaField\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Create dataset if it doesn't exist\n",
        "dataset_ref = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
        "try:\n",
        "    client.get_dataset(DATASET_ID)\n",
        "    print(f\"Dataset '{DATASET_ID}' already exists\")\n",
        "except Exception:\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\"\n",
        "    client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"Created dataset '{DATASET_ID}'\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGp2H7voNUpJ"
      },
      "source": [
        "# Define schema for collisions table with column descriptions\n",
        "collisions_schema = [\n",
        "    SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\",\n",
        "                description=\"Unique identifier for the collision\"),\n",
        "    SchemaField(\"DATE_TIME\", \"TIMESTAMP\", mode=\"REQUIRED\",\n",
        "                description=\"Date and time of the collision\"),\n",
        "    SchemaField(\"LATITUDE\", \"FLOAT\",\n",
        "                description=\"Latitude of the collision location\"),\n",
        "    SchemaField(\"LONGITUDE\", \"FLOAT\",\n",
        "                description=\"Longitude of the collision location\"),\n",
        "    SchemaField(\"ZIPCODE\", \"STRING\",\n",
        "                description=\"ZIP code (detected from coordinates, or reported)\"),\n",
        "    SchemaField(\"NEIGHBORHOOD\", \"STRING\",\n",
        "                description=\"NYC neighborhood (detected from coordinates)\"),\n",
        "    SchemaField(\"BOROUGH\", \"STRING\",\n",
        "                description=\"NYC borough (detected from coordinates, or reported)\"),\n",
        "    SchemaField(\"PERSONS_INJURED\", \"INTEGER\",\n",
        "                description=\"Total number of persons injured\"),\n",
        "    SchemaField(\"PERSONS_KILLED\", \"INTEGER\",\n",
        "                description=\"Total number of persons killed\"),\n",
        "    SchemaField(\"PEDESTRIANS_INJURED\", \"INTEGER\",\n",
        "                description=\"Number of pedestrians injured\"),\n",
        "    SchemaField(\"PEDESTRIANS_KILLED\", \"INTEGER\",\n",
        "                description=\"Number of pedestrians killed\"),\n",
        "    SchemaField(\"CYCLISTS_INJURED\", \"INTEGER\",\n",
        "                description=\"Number of cyclists injured\"),\n",
        "    SchemaField(\"CYCLISTS_KILLED\", \"INTEGER\",\n",
        "                description=\"Number of cyclists killed\"),\n",
        "    SchemaField(\"MOTORISTS_INJURED\", \"INTEGER\",\n",
        "                description=\"Number of motorists injured\"),\n",
        "    SchemaField(\"MOTORISTS_KILLED\", \"INTEGER\",\n",
        "                description=\"Number of motorists killed\"),\n",
        "    SchemaField(\"ON_STREET_NAME\", \"STRING\",\n",
        "                description=\"Name of the street where the collision occurred\"),\n",
        "    SchemaField(\"CROSS_STREET_NAME\", \"STRING\",\n",
        "                description=\"Name of the cross street\"),\n",
        "    SchemaField(\"OFF_STREET_NAME\", \"STRING\",\n",
        "                description=\"Name of the off-street location\"),\n",
        "    SchemaField(\"REPORTED_ZIPCODE\", \"STRING\",\n",
        "                description=\"Originally reported ZIP code\"),\n",
        "    SchemaField(\"REPORTED_BOROUGH\", \"STRING\",\n",
        "                description=\"Originally reported borough\"),\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWhcyYGdNUpJ"
      },
      "source": [
        "# Define schema for causes_types table\n",
        "causes_types_schema = [\n",
        "    SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\",\n",
        "                description=\"Collision ID (foreign key to collisions table)\"),\n",
        "    SchemaField(\"CAUSE\", \"STRING\",\n",
        "                description=\"Contributing factor for the vehicle\"),\n",
        "    SchemaField(\"VEHICLE_TYPE\", \"STRING\",\n",
        "                description=\"Type of vehicle involved\"),\n",
        "    SchemaField(\"VEHICLE\", \"INTEGER\",\n",
        "                description=\"Vehicle number in the collision (1-5)\"),\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijaqRLFNUpK"
      },
      "source": [
        "%%time\n",
        "# Upload collisions table\n",
        "collisions_table_id = f\"{PROJECT_ID}.{DATASET_ID}.collisions\"\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=collisions_schema,\n",
        "    write_disposition='WRITE_TRUNCATE'  # Replace existing table\n",
        ")\n",
        "\n",
        "job = client.load_table_from_dataframe(final_df, collisions_table_id, job_config=job_config)\n",
        "job.result()  # Wait for completion\n",
        "\n",
        "print(f\"Loaded {job.output_rows:,} rows into {collisions_table_id}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMBpqSgCNUpK"
      },
      "source": [
        "%%time\n",
        "# Upload causes_types table\n",
        "causes_types_table_id = f\"{PROJECT_ID}.{DATASET_ID}.causes_types\"\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=causes_types_schema,\n",
        "    write_disposition='WRITE_TRUNCATE'  # Replace existing table\n",
        ")\n",
        "\n",
        "job = client.load_table_from_dataframe(vehicle_causes, causes_types_table_id, job_config=job_config)\n",
        "job.result()  # Wait for completion\n",
        "\n",
        "print(f\"Loaded {job.output_rows:,} rows into {causes_types_table_id}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReSDaYzNUpK"
      },
      "source": [
        "# Add primary key constraint (not enforced, for documentation)\n",
        "# Run these SQL commands in BigQuery console or uncomment to execute:\n",
        "pk_fk_sql = '''\n",
        "-- Define primary key for collisions table (not enforced)\n",
        "ALTER TABLE `{project}.{dataset}.collisions`\n",
        "ADD PRIMARY KEY (UNIQUE_KEY) NOT ENFORCED;\n",
        "\n",
        "-- Define foreign key for causes_types table (not enforced)\n",
        "ALTER TABLE `{project}.{dataset}.causes_types`\n",
        "ADD FOREIGN KEY (UNIQUE_KEY)\n",
        "REFERENCES `{project}.{dataset}.collisions`(UNIQUE_KEY) NOT ENFORCED;\n",
        "'''.format(project=PROJECT_ID, dataset=DATASET_ID)\n",
        "\n",
        "print(\"SQL for primary/foreign key constraints:\")\n",
        "print(pk_fk_sql)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R25U2GN3NUpK"
      },
      "source": [
        "# Grant read access to all authenticated users\n",
        "dataset = client.get_dataset(DATASET_ID)\n",
        "access_entries = list(dataset.access_entries)\n",
        "\n",
        "# Check if already granted\n",
        "already_granted = any(\n",
        "    e.entity_id == 'allAuthenticatedUsers'\n",
        "    for e in access_entries\n",
        ")\n",
        "\n",
        "if not already_granted:\n",
        "    access_entries.append(\n",
        "        bigquery.AccessEntry(\n",
        "            role='READER',\n",
        "            entity_type='specialGroup',\n",
        "            entity_id='allAuthenticatedUsers',\n",
        "        )\n",
        "    )\n",
        "    dataset.access_entries = access_entries\n",
        "    client.update_dataset(dataset, ['access_entries'])\n",
        "    print(f\"Granted read access to allAuthenticatedUsers\")\n",
        "else:\n",
        "    print(\"Read access already granted to allAuthenticatedUsers\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA_UwECQNUpL"
      },
      "source": [
        "## 8. Pipeline Complete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLlBeb2eNUpL"
      },
      "source": [
        "# Calculate total runtime\n",
        "t_end = datetime.now()\n",
        "runtime = t_end - t_start\n",
        "\n",
        "# Calculate retention statistics\n",
        "original_count = ORIGINAL_RECORD_COUNT  # Original loaded data\n",
        "final_count = len(final_df)  # After all filtering\n",
        "retention_pct = 100 * final_count / original_count\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"NYC COLLISIONS DATA PIPELINE COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n⏱️  RUNTIME\")\n",
        "print(f\"   Started:  {t_start}\")\n",
        "print(f\"   Finished: {t_end}\")\n",
        "print(f\"   Duration: {runtime}\")\n",
        "\n",
        "print(f\"\\n📊 DATA RETENTION\")\n",
        "print(f\"   Original records loaded:     {original_count:>12,}\")\n",
        "print(f\"   Records excluded (quality):  {len(records_to_exclude):>12,}\")\n",
        "print(f\"   Records excluded (geocoding):{original_count - len(gdf_nyc):>12,}\")\n",
        "print(f\"   Records excluded (duplicates):{len(duplicate_keys):>12,}\")\n",
        "print(f\"   ─────────────────────────────────────────────\")\n",
        "print(f\"   Final records exported:      {final_count:>12,}  ({retention_pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\n📁 BIGQUERY TABLES\")\n",
        "print(f\"   {PROJECT_ID}.{DATASET_ID}.collisions     → {len(final_df):,} rows\")\n",
        "print(f\"   {PROJECT_ID}.{DATASET_ID}.causes_types   → {len(vehicle_causes):,} rows\")\n",
        "\n",
        "print(f\"\\n🔍 SAMPLE QUERY\")\n",
        "print(f\"\"\"\n",
        "   SELECT\n",
        "     DATE(DATE_TIME) as date,\n",
        "     BOROUGH,\n",
        "     SUM(PERSONS_INJURED) as total_injured,\n",
        "     SUM(PERSONS_KILLED) as total_killed\n",
        "   FROM `{PROJECT_ID}.{DATASET_ID}.collisions`\n",
        "   GROUP BY 1, 2\n",
        "   ORDER BY total_killed DESC\n",
        "   LIMIT 10\n",
        "\"\"\")\n",
        "print(\"=\" * 70)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}